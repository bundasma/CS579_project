{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 168)\n",
      "   object_id    fluxmean       std0        std1        std2        std3  \\\n",
      "0        615 -123.096998  83.944735  601.787302  455.121346  335.425053   \n",
      "1        713   -1.423351   7.113509    5.712334    5.770738    6.450413   \n",
      "2        730    2.267434   1.828872    1.807229    5.559483    8.191987   \n",
      "3        745    8.909206   4.374445   25.964659   31.957997   34.967698   \n",
      "4       1124    7.145702   2.360084    8.107525   21.319854   26.270649   \n",
      "\n",
      "         std4        std5    median0     median1  ...  meanratio2  meanratio3  \\\n",
      "0  291.803449  294.779522 -10.015225 -488.057969  ...    1.093356    0.966500   \n",
      "1    6.406989    7.094073  -3.096804   -0.561735  ...    0.535085    0.682655   \n",
      "2   10.710344   13.332758   0.024093    0.171336  ...    0.985800    1.426632   \n",
      "3   33.069053   26.060130   1.056714    0.888115  ...    1.060016    1.753098   \n",
      "4   26.865913   21.434627   0.581027    1.154596  ...    1.543464    1.713935   \n",
      "\n",
      "   meanratio4  meanratio5  stdratio0  stdratio1  stdratio2  stdratio3  \\\n",
      "0    0.404498    0.338850   0.212098   2.059400   1.415407   0.970875   \n",
      "1    0.615304    1.396969   1.131480   0.869835   0.880294   1.004811   \n",
      "2    2.097847    2.260939   0.230908   0.228050   0.774923   1.232297   \n",
      "3    1.549402    1.200047   0.143878   0.995354   1.284114   1.439873   \n",
      "4    1.477515    0.943926   0.113467   0.412592   1.253537   1.640111   \n",
      "\n",
      "   stdratio4  stdratio5  \n",
      "0   0.823811   0.833614  \n",
      "1   0.996698   1.127691  \n",
      "2   1.743201   2.372553  \n",
      "3   1.340729   0.999745  \n",
      "4   1.689834   1.261988  \n",
      "\n",
      "[5 rows x 168 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from scipy.stats import kurtosis, median_absolute_deviation, iqr\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from runpbmet import dopbstuff \n",
    "from gatspy.periodic import LombScargleMultiband\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the period of time series data, takes too long to actually use though.\n",
    "\n",
    "def get_period(data):\n",
    "\n",
    "\n",
    "    model = LombScargleMultiband(fit_period=True)\n",
    "\n",
    "    t_min = 0.1\n",
    "    t_max = 1\n",
    "\n",
    "    model.optimizer.set(period_range=(t_min, t_max), first_pass_coverage=5)\n",
    "\n",
    "    model.fit(data['mjd'], data['flux'], dy=data['flux_err'], filts=data['passband'])\n",
    "    period = model.best_period\n",
    "\n",
    "    print(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://www.kaggle.com/meaninglesslives/simple-neural-net-for-time-series-classification\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize = (12,12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the wtable, which contains the proprortion of occurence of each label in a label set\n",
    "\n",
    "def get_wtable(labels):\n",
    "    \n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    occur = dict(zip(unique, counts))\n",
    "\n",
    "    num_classes = len(np.unique(labels))\n",
    "    wtable = np.zeros(num_classes)\n",
    "\n",
    "    #wtable - is a numpy 1d array with (the number of times class y_true occur in the data set)/(size of data set)\n",
    "    for i in range(len(wtable)):\n",
    "\n",
    "        wtable[i] = occur[unique[i]]/len(labels)\n",
    "        \n",
    "    return wtable\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate of relative amplitude of flux\n",
    "\n",
    "def percent_amp(flux):\n",
    "    \n",
    "    min_flux = np.min(flux)\n",
    "    max_flux = np.max(flux)\n",
    "    med_flux = np.median(flux)\n",
    "    \n",
    "    \n",
    "    return np.max([(min_flux/med_flux),(max_flux/med_flux)])\n",
    "    \n",
    "#ratio of difference between 95th and 5th percentiles and the median flux\n",
    "def percent_diff_flux_p(flux):\n",
    "    \n",
    "    fifth = np.percentile(flux,5)\n",
    "    ninetyfifth = np.percentile(flux,95)\n",
    "    med_flux = np.median(flux)\n",
    "    \n",
    "    return((ninetyfifth-fifth)/med_flux)\n",
    " \n",
    "#return variuous percentiles of flux    \n",
    "def per_10(flux):\n",
    "    \n",
    "    return(np.percentile(flux,10))\n",
    "    \n",
    "def per_25(flux):\n",
    "    \n",
    "    return(np.percentile(flux,25))\n",
    "\n",
    "def per_50(flux):\n",
    "    \n",
    "    return(np.percentile(flux,50))\n",
    "\n",
    "def per_75(flux):\n",
    "    \n",
    "    return(np.percentile(flux,75))\n",
    "\n",
    "def per_90(flux):\n",
    "    \n",
    "    return(np.percentile(flux,90))\n",
    "  \n",
    "def per_95(flux):\n",
    "    \n",
    "    return(np.percentile(flux,95))    \n",
    "    \n",
    "    \n",
    "#finds largest/smallest jump/dip in flux    \n",
    "def max_slope(flux):\n",
    "    \n",
    "    gradient = np.gradient(flux)\n",
    "    \n",
    "    return(np.max(gradient))\n",
    "\n",
    "def min_slope(flux):\n",
    "    \n",
    "    gradient = np.gradient(flux)\n",
    "    \n",
    "    return(np.max(gradient))\n",
    "\n",
    "\n",
    "#finds percent of observations where detected = 1\n",
    "def percent_detected(detected):\n",
    "    \n",
    "    numdetected = np.sum(detected)\n",
    "    \n",
    "    return numdetected/len(detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "#does major feature engineering\n",
    "def do_aggs(measurements):\n",
    "    \n",
    "    preagg = measurements.copy()\n",
    "    \n",
    "    #inspired by https://www.kaggle.com/meaninglesslives/simple-neural-net-for-time-series-classification\n",
    "    measurements['flux_ratio_sq'] = np.power(measurements['flux'] / measurements['flux_err'], 2.0)\n",
    "    measurements['flux_by_flux_ratio_sq'] = measurements['flux'] * measurements['flux_ratio_sq']\n",
    "    \n",
    "    \n",
    "    #rolling mean of previous 10 observations\n",
    "    measurements['rollingmeanflux'] = measurements[\"flux\"].rolling(10,min_periods = 1).mean()\n",
    "    \n",
    "    print(\"doing first batch of aggregations\")\n",
    "    \n",
    "    \n",
    "    #do mass feature engineering on various elements\n",
    "    aggs = {\n",
    "    \n",
    "    'flux': ['max', 'min','median','mean',np.std,\"skew\",percent_amp,kurtosis,max_slope,min_slope,median_absolute_deviation,iqr,per_10,per_25,per_50,per_75,per_90,per_95],\n",
    "    'flux_err' : [\"mean\",\"max\",\"min\",np.std,\"skew\",percent_amp,kurtosis,max_slope,min_slope,median_absolute_deviation,iqr,per_10,per_25,per_50,per_75,per_90,per_95],\n",
    "    'passband': ['mean',\"median\",\"std\"],\n",
    "    'mjd': ['max','min',np.std],\n",
    "    'detected': [percent_detected,\"std\"],\n",
    "    'flux_ratio_sq': ['skew',\"mean\",\"median\"],\n",
    "    'flux_by_flux_ratio_sq':['skew',\"mean\",\"median\"],\n",
    "    'rollingmeanflux' : ['mean',\"skew\",kurtosis,max_slope,min_slope]\n",
    "\n",
    "    }\n",
    "\n",
    "    agg_df = measurements.groupby('object_id', as_index = False).agg(aggs)\n",
    "    \n",
    "    #fix column names\n",
    "    agg_df.columns = list(map(''.join, agg_df.columns.values))\n",
    "     \n",
    "    \n",
    "    #ratio of maximum flux to mean flux\n",
    "    agg_df[\"max_mean\"] = agg_df[\"fluxmax\"] / agg_df[\"fluxmean\"]\n",
    "    \n",
    "    \n",
    "    print(\"doing second batch of aggregations, passbands\")\n",
    "    withpb = dopbstuff(preagg)\n",
    "    \n",
    "    print(\"done with hard engineering\")\n",
    "    \n",
    "    agg_df = pd.merge(agg_df,withpb, on =\"object_id\")\n",
    "    \n",
    "    \n",
    "    return agg_df\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "#processes data, returns various forms of labels     \n",
    "def get_labels(full_df):\n",
    "    \n",
    "    #create train labels\n",
    "\n",
    "    unique_labels = np.sort(full_df[\"target\"].unique())\n",
    "    all_labels = full_df[\"target\"]\n",
    "    empty_row = np.zeros(len(unique_labels))\n",
    "\n",
    "    unique_labels_dict = {6:0, 15:1, 16:2, 42:3, 52:4, 53:5, 62:6, 64:7, 65:8, 67:9, 88:10, 90:11, 92:12, 95:13}\n",
    "    my_train_labels = []\n",
    "\n",
    "    for label in all_labels:\n",
    "\n",
    "        newrow = empty_row.copy()\n",
    "\n",
    "        newrow[unique_labels_dict[label]] = 1\n",
    "\n",
    "        my_train_labels.append(newrow)\n",
    "    \n",
    "    return my_train_labels, unique_labels, all_labels\n",
    "\n",
    "\n",
    "\n",
    "#processes data under assumption it is training data\n",
    "def process_train(measurements,metadata):\n",
    "    \n",
    "   \n",
    "    df = do_aggs(measurements)\n",
    "    \n",
    "    premerge = df\n",
    "    \n",
    "    df_2 = pd.merge(df,metadata, on =\"object_id\")\n",
    "    \n",
    "\n",
    "    values = {'distmod': 0}\n",
    "    df_2 = df_2.fillna(value=values)\n",
    "    \n",
    "    labels, unique_labels, all_labs = get_labels(df_2)\n",
    "    \n",
    "    ids = df_2[\"object_id\"]\n",
    "    \n",
    "    df_2 = df_2.drop(columns = [\"object_id\",\"target\",\"mjdmax\",\"mjdmin\",\"mwebv\"])#,\"flux_by_flux_ratio_sqsum\",\"flux_ratio_sqsum\"])#,\"hostgal_photoz\",\"hostgal_photoz_err\",\"mwebv\"])\n",
    "\n",
    "    df_2=tf.keras.utils.normalize(df_2, axis=1)\n",
    "    #df_2 =  sk.preprocessing.normalize(df_2)\n",
    "    \n",
    "    return df_2, ids, labels, unique_labels, all_labs, premerge\n",
    "\n",
    "\n",
    "#processes data under assumption it is test data\n",
    "\n",
    "def process_test(measurements,metadata):\n",
    "\n",
    "    #do aggregation\n",
    "    print(\"doing agg\")\n",
    "    df = do_aggs(measurements)\n",
    "     \n",
    "    \n",
    "    #merge with metadata\n",
    "    print(\"doing merge\")\n",
    "    df_2 = pd.merge(df,metadata, on =\"object_id\")\n",
    "    \n",
    "    #values = {'distmod': 0, 'hostgal_specz' : 0}\n",
    "    df_2 = df_2.fillna(0)\n",
    "    \n",
    "    ids = df_2[\"object_id\"]\n",
    "    df_2 = df_2.drop(columns = [\"object_id\",\"mjdmax\",\"mjdmin\",\"mwebv\"])#,\"flux_by_flux_ratio_sqsum\"])#,\"hostgal_photoz\",\"hostgal_photoz_err\",\"mwebv\"])\n",
    "    df_2 = df_2.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    df_2=tf.keras.utils.normalize(df_2, axis=1)\n",
    "    \n",
    "    #df_2 =  sk.preprocessing.normalize(df_2)\n",
    "    \n",
    "    return df_2, ids\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles making predictions, returns a dataframe contaning all preditions including class 99\n",
    "\n",
    "def make_predictions(data,ids,model):\n",
    "    \n",
    "    \n",
    "\n",
    "    prediction = model.predict(data)\n",
    "\n",
    "    df_labels = [\"class_6\",\"class_15\",\"class_16\",\"class_42\",\"class_52\",\"class_53\",\"class_62\",\"class_64\",\"class_65\",\"class_67\",\"class_88\",\"class_90\",\"class_92\",\"class_95\"]\n",
    "    pred_df = pd.DataFrame(prediction,columns = df_labels)\n",
    "\n",
    "    nrows = pred_df.shape[0]\n",
    "    class_99_preds = np.zeros(nrows)\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(nrows):\n",
    "        class_99_preds[i] = np.max(prediction[i,1:14])/14\n",
    "\n",
    "  \n",
    "    \n",
    "    pred_df.insert(loc=0, column='object_id', value=ids)\n",
    "    pred_df.insert(loc=pred_df.shape[1], column='class_99', value=class_99_preds)\n",
    "    \n",
    "    \n",
    "    return pred_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69795\n",
    "def mywloss(y_true,y_pred):  \n",
    "    \n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    \n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.savefig(\"learning.png\")\n",
    "    #plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.savefig(\"learning.png\")\n",
    "   # plt.show()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(filename,batchnums,model):\n",
    "    \n",
    "    \n",
    "    test_meta = pd.read_csv(\"./test_set_metadata.csv\")\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    for bn in batchnums:\n",
    "        \n",
    "        print(\"starting bn = \", bn)\n",
    "        testcsv = \"test_set_batch\" + str(bn) + \".csv\"\n",
    "        \n",
    "        print(\"starting read\")\n",
    "        test_set = pd.read_csv(testcsv)\n",
    "        \n",
    "        print(\"starting process\")\n",
    "        full_test, test_ids =  process_test(test_set,test_meta)\n",
    "        \n",
    "        print(\"starting prediction\")\n",
    "        prediction = make_predictions(full_test,test_ids,model)\n",
    "        \n",
    "        if(first == True):\n",
    "            prediction.to_csv(filename,index = False)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            prediction.to_csv(filename,index = False,mode = 'a',header = False)\n",
    "        \n",
    "        first = False\n",
    "        \n",
    "        print(\"done wiht bn = \", bn)\n",
    "        print()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots lightcurve of specific object data\n",
    "\n",
    "\n",
    "def make_lightcurve(data):\n",
    "    \n",
    "\n",
    "\n",
    "    mjd = data[\"mjd\"]\n",
    "\n",
    "    band_0 = data[\"flux\"][data[\"passband\"] == 0]\n",
    "    mjd0 = data[\"mjd\"][data[\"passband\"] == 0]\n",
    "\n",
    "    band_1 = data[\"flux\"][data[\"passband\"] == 1]\n",
    "    mjd1 = data[\"mjd\"][data[\"passband\"] == 1]\n",
    "\n",
    "    band_2 = data[\"flux\"][data[\"passband\"] == 2]\n",
    "    mjd2 = data[\"mjd\"][data[\"passband\"] == 2]\n",
    "\n",
    "    band_3 = data[\"flux\"][data[\"passband\"] == 3]\n",
    "    mjd3 = data[\"mjd\"][data[\"passband\"] == 3]\n",
    "\n",
    "    band_4 = data[\"flux\"][data[\"passband\"] == 4]\n",
    "    mjd4 = data[\"mjd\"][data[\"passband\"] == 4]\n",
    "\n",
    "    band_5 = data[\"flux\"][data[\"passband\"] == 5]\n",
    "    mjd5 = data[\"mjd\"][data[\"passband\"] == 5]\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.scatter(mjd0,band_0,color = \"purple\", label = \"u - 0\")\n",
    "    plt.scatter(mjd1,band_1, color = \"green\", label = \"g - 1\")\n",
    "    plt.scatter(mjd2,band_2, color = \"red\", label = \"r - 2\")\n",
    "    plt.scatter(mjd3,band_3, color = \"yellow\", label = \"i - 3\")\n",
    "    plt.scatter(mjd4,band_4, color = \"brown\", label = \"z - 4 \")\n",
    "    plt.scatter(mjd5,band_5, color = \"black\", label = \"y - 5\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning(learning, save = False, accfn = \"learningacc.png\", lossfn = \"learningloss.png\"):\n",
    "    \n",
    "    plt.figure(figsize = (12,8))\n",
    "    \n",
    "    plt.plot(learning.history[\"acc\"][1:], color = \"green\", label = \"training\")\n",
    "    plt.plot(learning.history['val_acc'][1:], color = \"blue\", label = \"validation\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"epoch\",fontsize = 18)\n",
    "    plt.xticks(fontsize = 16)\n",
    "    \n",
    "    \n",
    "    plt.ylabel(\"accuracy\", fontsize = 18)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    \n",
    "    plt.title(\"model learning - accuracy\", fontsize = 22)\n",
    "    \n",
    "    plt.legend(fontsize = 16)\n",
    "    \n",
    "    if(save == False):\n",
    "        plt.show()\n",
    "        \n",
    "    if(save == True):\n",
    "        plt.savefig(accfn, dpi = 600)\n",
    "        \n",
    "        \n",
    "    plt.figure(figsize = (12,8))\n",
    "    \n",
    "    plt.plot(learning.history[\"loss\"][1:], color = \"green\", label = \"training\")\n",
    "    plt.plot(learning.history['val_loss'][1:], color = \"blue\", label = \"validation\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"epoch\",fontsize = 18)\n",
    "    plt.xticks(fontsize = 16)\n",
    "    \n",
    "    \n",
    "    plt.ylabel(\"loss\", fontsize = 18)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    \n",
    "    plt.title(\"model learning - loss\", fontsize = 22)\n",
    "    \n",
    "    plt.legend(fontsize = 16)\n",
    "    \n",
    "    if(save == False):\n",
    "        plt.show()\n",
    "        \n",
    "    if(save == True):\n",
    "        plt.savefig(lossfn, dpi = 600)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(full_train):\n",
    "    \n",
    "    num_neurons = 128*4\n",
    "    num_features = np.shape(full_train)[1]\n",
    "    num_classes = 14\n",
    "    drop_rate = 0.40\n",
    "    \n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "    \n",
    "      tf.keras.layers.Dense(num_neurons, input_dim = num_features, activation='sigmoid'),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Dropout(drop_rate),\n",
    "\n",
    "      tf.keras.layers.Dense(num_neurons, input_dim = num_features, activation='sigmoid'),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Dropout(drop_rate), \n",
    "\n",
    "      tf.keras.layers.Dense(num_neurons//2, input_dim = num_features, activation='sigmoid'),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Dropout(drop_rate), \n",
    "\n",
    "      tf.keras.layers.Dense(num_neurons//4, input_dim = num_features, activation='sigmoid'),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Dropout(drop_rate), \n",
    "\n",
    "      tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "train_set = pd.read_csv(\"training_set.csv\")\n",
    "train_meta = pd.read_csv(\"training_set_metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing first batch of aggregations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing second batch of aggregations, passbands\n",
      "done with hard engineering\n"
     ]
    }
   ],
   "source": [
    "full_train, train_ids, train_labels, unique_train_labels, all_labs, premerge = process_train(train_set,train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 6278 samples, validate on 1570 samples\n",
      "WARNING:tensorflow:From C:\\Users\\15173\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/150\n",
      "6278/6278 [==============================] - 3s 490us/sample - loss: 3.0186 - acc: 0.1448 - val_loss: 1.9953 - val_acc: 0.0981\n",
      "Epoch 2/150\n",
      "6278/6278 [==============================] - 2s 367us/sample - loss: 2.5757 - acc: 0.1803 - val_loss: 1.8453 - val_acc: 0.1707\n",
      "Epoch 3/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 2.4099 - acc: 0.2212 - val_loss: 1.8450 - val_acc: 0.2115\n",
      "Epoch 4/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 2.2754 - acc: 0.2502 - val_loss: 1.5746 - val_acc: 0.3312\n",
      "Epoch 5/150\n",
      "6278/6278 [==============================] - 2s 377us/sample - loss: 2.1718 - acc: 0.2541 - val_loss: 1.6201 - val_acc: 0.3032\n",
      "Epoch 6/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 2.1456 - acc: 0.2737 - val_loss: 1.5580 - val_acc: 0.3242\n",
      "Epoch 7/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 2.0191 - acc: 0.3012 - val_loss: 1.6113 - val_acc: 0.3344\n",
      "Epoch 8/150\n",
      "6278/6278 [==============================] - 2s 375us/sample - loss: 1.9583 - acc: 0.3171 - val_loss: 1.6728 - val_acc: 0.3191\n",
      "Epoch 9/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.9387 - acc: 0.3162 - val_loss: 1.6366 - val_acc: 0.2866\n",
      "Epoch 10/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.8896 - acc: 0.3331 - val_loss: 1.5537 - val_acc: 0.3561\n",
      "Epoch 11/150\n",
      "6278/6278 [==============================] - 2s 376us/sample - loss: 1.8984 - acc: 0.3206 - val_loss: 1.6769 - val_acc: 0.3586\n",
      "Epoch 12/150\n",
      "6278/6278 [==============================] - 2s 375us/sample - loss: 1.9008 - acc: 0.3243 - val_loss: 1.8125 - val_acc: 0.3433\n",
      "Epoch 13/150\n",
      "6278/6278 [==============================] - 2s 369us/sample - loss: 1.8403 - acc: 0.3519 - val_loss: 1.5254 - val_acc: 0.3592\n",
      "Epoch 14/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.8321 - acc: 0.3487 - val_loss: 1.5243 - val_acc: 0.2490\n",
      "Epoch 15/150\n",
      "6278/6278 [==============================] - 2s 370us/sample - loss: 1.7996 - acc: 0.3479 - val_loss: 1.5853 - val_acc: 0.4376\n",
      "Epoch 16/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.7747 - acc: 0.3552 - val_loss: 1.5291 - val_acc: 0.3312\n",
      "Epoch 17/150\n",
      "6278/6278 [==============================] - 2s 375us/sample - loss: 1.7715 - acc: 0.3592 - val_loss: 1.4146 - val_acc: 0.3968\n",
      "Epoch 18/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.7270 - acc: 0.3622 - val_loss: 1.5154 - val_acc: 0.3459\n",
      "Epoch 19/150\n",
      "6278/6278 [==============================] - 2s 372us/sample - loss: 1.7403 - acc: 0.3643 - val_loss: 1.4445 - val_acc: 0.4892\n",
      "Epoch 20/150\n",
      "6278/6278 [==============================] - 2s 369us/sample - loss: 1.7664 - acc: 0.3705 - val_loss: 1.5309 - val_acc: 0.3790\n",
      "Epoch 21/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.7402 - acc: 0.3657 - val_loss: 1.5629 - val_acc: 0.3688\n",
      "Epoch 22/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.7438 - acc: 0.3781 - val_loss: 1.5532 - val_acc: 0.3255\n",
      "Epoch 23/150\n",
      "6278/6278 [==============================] - 2s 376us/sample - loss: 1.7064 - acc: 0.3708 - val_loss: 1.5455 - val_acc: 0.3325\n",
      "Epoch 24/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.7242 - acc: 0.3766 - val_loss: 1.7267 - val_acc: 0.3662\n",
      "Epoch 25/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.6903 - acc: 0.3692 - val_loss: 1.4670 - val_acc: 0.3420\n",
      "Epoch 26/150\n",
      "6278/6278 [==============================] - 2s 375us/sample - loss: 1.6778 - acc: 0.3812 - val_loss: 1.3909 - val_acc: 0.3764\n",
      "Epoch 27/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.6777 - acc: 0.3820 - val_loss: 1.4758 - val_acc: 0.3191\n",
      "Epoch 28/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.6932 - acc: 0.3691 - val_loss: 1.4383 - val_acc: 0.4025\n",
      "Epoch 29/150\n",
      "6278/6278 [==============================] - 2s 375us/sample - loss: 1.6741 - acc: 0.3906 - val_loss: 1.4879 - val_acc: 0.3083\n",
      "Epoch 30/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.6444 - acc: 0.3781 - val_loss: 1.3826 - val_acc: 0.3459\n",
      "Epoch 31/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.6212 - acc: 0.3721 - val_loss: 1.4461 - val_acc: 0.3885\n",
      "Epoch 32/150\n",
      "6278/6278 [==============================] - 2s 377us/sample - loss: 1.6303 - acc: 0.3891 - val_loss: 1.3453 - val_acc: 0.3955\n",
      "Epoch 33/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.6576 - acc: 0.3910 - val_loss: 1.4602 - val_acc: 0.3433\n",
      "Epoch 34/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.6416 - acc: 0.3845 - val_loss: 1.4145 - val_acc: 0.3720\n",
      "Epoch 35/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.6525 - acc: 0.3831 - val_loss: 1.3824 - val_acc: 0.4146\n",
      "Epoch 36/150\n",
      "6278/6278 [==============================] - 2s 376us/sample - loss: 1.6174 - acc: 0.3879 - val_loss: 1.3282 - val_acc: 0.3675\n",
      "Epoch 37/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.6294 - acc: 0.3998 - val_loss: 1.3602 - val_acc: 0.3102\n",
      "Epoch 38/150\n",
      "6278/6278 [==============================] - 2s 375us/sample - loss: 1.5764 - acc: 0.3974 - val_loss: 1.4269 - val_acc: 0.3732\n",
      "Epoch 39/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.6007 - acc: 0.3968 - val_loss: 1.4581 - val_acc: 0.2446\n",
      "Epoch 40/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.5913 - acc: 0.3898 - val_loss: 1.4642 - val_acc: 0.2701\n",
      "Epoch 41/150\n",
      "6278/6278 [==============================] - 2s 382us/sample - loss: 1.6158 - acc: 0.3885 - val_loss: 1.4477 - val_acc: 0.2624\n",
      "Epoch 42/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.6041 - acc: 0.3912 - val_loss: 1.3628 - val_acc: 0.2675\n",
      "Epoch 43/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.5795 - acc: 0.3840 - val_loss: 1.4406 - val_acc: 0.3459\n",
      "Epoch 44/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.6087 - acc: 0.3817 - val_loss: 1.3817 - val_acc: 0.3892\n",
      "Epoch 45/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.6156 - acc: 0.4006 - val_loss: 1.4970 - val_acc: 0.3191\n",
      "Epoch 46/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.5960 - acc: 0.4033 - val_loss: 1.3824 - val_acc: 0.3433\n",
      "Epoch 47/150\n",
      "6278/6278 [==============================] - 2s 376us/sample - loss: 1.5941 - acc: 0.3831 - val_loss: 1.2864 - val_acc: 0.4656\n",
      "Epoch 48/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.5684 - acc: 0.4025 - val_loss: 1.3358 - val_acc: 0.3675\n",
      "Epoch 49/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.5537 - acc: 0.3984 - val_loss: 1.5200 - val_acc: 0.3682\n",
      "Epoch 50/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.5549 - acc: 0.4092 - val_loss: 1.3958 - val_acc: 0.3395\n",
      "Epoch 51/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.5726 - acc: 0.4000 - val_loss: 1.4088 - val_acc: 0.3510\n",
      "Epoch 52/150\n",
      "6278/6278 [==============================] - 2s 377us/sample - loss: 1.6122 - acc: 0.4027 - val_loss: 1.3511 - val_acc: 0.2981\n",
      "Epoch 53/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.5650 - acc: 0.3938 - val_loss: 1.4024 - val_acc: 0.4019\n",
      "Epoch 54/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.5624 - acc: 0.4070 - val_loss: 1.3666 - val_acc: 0.4312\n",
      "Epoch 55/150\n",
      "6278/6278 [==============================] - 2s 366us/sample - loss: 1.5468 - acc: 0.3977 - val_loss: 1.3347 - val_acc: 0.4465\n",
      "Epoch 56/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.5334 - acc: 0.4116 - val_loss: 1.2825 - val_acc: 0.4809\n",
      "Epoch 57/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.5524 - acc: 0.3969 - val_loss: 1.3861 - val_acc: 0.3535\n",
      "Epoch 58/150\n",
      "6278/6278 [==============================] - 2s 372us/sample - loss: 1.5437 - acc: 0.4016 - val_loss: 1.3132 - val_acc: 0.3968\n",
      "Epoch 59/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.5566 - acc: 0.4032 - val_loss: 1.2900 - val_acc: 0.3459\n",
      "Epoch 60/150\n",
      "6278/6278 [==============================] - 2s 368us/sample - loss: 1.5704 - acc: 0.3976 - val_loss: 1.3630 - val_acc: 0.3701\n",
      "Epoch 61/150\n",
      "6278/6278 [==============================] - 2s 377us/sample - loss: 1.5570 - acc: 0.4086 - val_loss: 1.3441 - val_acc: 0.3873\n",
      "Epoch 62/150\n",
      "6278/6278 [==============================] - 2s 367us/sample - loss: 1.5247 - acc: 0.4124 - val_loss: 1.2943 - val_acc: 0.3994\n",
      "Epoch 63/150\n",
      "6278/6278 [==============================] - 2s 376us/sample - loss: 1.5577 - acc: 0.4020 - val_loss: 1.3485 - val_acc: 0.3611\n",
      "Epoch 64/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.5154 - acc: 0.4094 - val_loss: 1.2636 - val_acc: 0.3752\n",
      "Epoch 65/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.4845 - acc: 0.4068 - val_loss: 1.2972 - val_acc: 0.4459\n",
      "Epoch 66/150\n",
      "6278/6278 [==============================] - 2s 370us/sample - loss: 1.5613 - acc: 0.4140 - val_loss: 1.4371 - val_acc: 0.2904\n",
      "Epoch 67/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.5030 - acc: 0.4153 - val_loss: 1.3482 - val_acc: 0.3401\n",
      "Epoch 68/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.5375 - acc: 0.4083 - val_loss: 1.3954 - val_acc: 0.3771\n",
      "Epoch 69/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.5169 - acc: 0.4119 - val_loss: 1.3197 - val_acc: 0.3522\n",
      "Epoch 70/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.5511 - acc: 0.4001 - val_loss: 1.3178 - val_acc: 0.3975\n",
      "Epoch 71/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.5095 - acc: 0.4159 - val_loss: 1.3488 - val_acc: 0.3529\n",
      "Epoch 72/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.5348 - acc: 0.4154 - val_loss: 1.3270 - val_acc: 0.3153\n",
      "Epoch 73/150\n",
      "6278/6278 [==============================] - 2s 367us/sample - loss: 1.5120 - acc: 0.4097 - val_loss: 1.2493 - val_acc: 0.3860\n",
      "Epoch 74/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.4996 - acc: 0.4137 - val_loss: 1.2984 - val_acc: 0.3363\n",
      "Epoch 75/150\n",
      "6278/6278 [==============================] - 2s 368us/sample - loss: 1.5167 - acc: 0.4038 - val_loss: 1.3014 - val_acc: 0.3605\n",
      "Epoch 76/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.5155 - acc: 0.4065 - val_loss: 1.3439 - val_acc: 0.3707\n",
      "Epoch 77/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.4913 - acc: 0.4094 - val_loss: 1.3232 - val_acc: 0.4338\n",
      "Epoch 78/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.5043 - acc: 0.4199 - val_loss: 1.3090 - val_acc: 0.2656\n",
      "Epoch 79/150\n",
      "6278/6278 [==============================] - 2s 371us/sample - loss: 1.4989 - acc: 0.3989 - val_loss: 1.2606 - val_acc: 0.3790\n",
      "Epoch 80/150\n",
      "6278/6278 [==============================] - 2s 370us/sample - loss: 1.5093 - acc: 0.4063 - val_loss: 1.3118 - val_acc: 0.3490\n",
      "Epoch 81/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.4734 - acc: 0.4114 - val_loss: 1.2772 - val_acc: 0.4344\n",
      "Epoch 82/150\n",
      "6278/6278 [==============================] - 2s 369us/sample - loss: 1.4855 - acc: 0.4124 - val_loss: 1.3775 - val_acc: 0.3599\n",
      "Epoch 83/150\n",
      "6278/6278 [==============================] - 2s 373us/sample - loss: 1.5018 - acc: 0.4100 - val_loss: 1.3387 - val_acc: 0.3350\n",
      "Epoch 84/150\n",
      "6278/6278 [==============================] - 2s 366us/sample - loss: 1.4832 - acc: 0.4094 - val_loss: 1.4493 - val_acc: 0.3713\n",
      "Epoch 85/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.4942 - acc: 0.4116 - val_loss: 1.3242 - val_acc: 0.4299\n",
      "Epoch 86/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.5095 - acc: 0.4073 - val_loss: 1.3088 - val_acc: 0.3529\n",
      "Epoch 87/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4957 - acc: 0.4078 - val_loss: 1.3133 - val_acc: 0.4089\n",
      "Epoch 88/150\n",
      "6278/6278 [==============================] - 3s 408us/sample - loss: 1.4955 - acc: 0.4178 - val_loss: 1.2886 - val_acc: 0.4108\n",
      "Epoch 89/150\n",
      "6278/6278 [==============================] - 3s 402us/sample - loss: 1.4959 - acc: 0.4175 - val_loss: 1.4205 - val_acc: 0.4076\n",
      "Epoch 90/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4642 - acc: 0.4204 - val_loss: 1.2258 - val_acc: 0.4573\n",
      "Epoch 91/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4854 - acc: 0.4269 - val_loss: 1.2675 - val_acc: 0.4586\n",
      "Epoch 92/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.4571 - acc: 0.4186 - val_loss: 1.3221 - val_acc: 0.4510\n",
      "Epoch 93/150\n",
      "6278/6278 [==============================] - 2s 393us/sample - loss: 1.5106 - acc: 0.4302 - val_loss: 1.3962 - val_acc: 0.3376\n",
      "Epoch 94/150\n",
      "6278/6278 [==============================] - 3s 401us/sample - loss: 1.4933 - acc: 0.4105 - val_loss: 1.3761 - val_acc: 0.3739\n",
      "Epoch 95/150\n",
      "6278/6278 [==============================] - 3s 407us/sample - loss: 1.4724 - acc: 0.4148 - val_loss: 1.3649 - val_acc: 0.3618\n",
      "Epoch 96/150\n",
      "6278/6278 [==============================] - 3s 405us/sample - loss: 1.5147 - acc: 0.4111 - val_loss: 1.3503 - val_acc: 0.4166\n",
      "Epoch 97/150\n",
      "6278/6278 [==============================] - 3s 408us/sample - loss: 1.4719 - acc: 0.4019 - val_loss: 1.3031 - val_acc: 0.4554\n",
      "Epoch 98/150\n",
      "6278/6278 [==============================] - 3s 415us/sample - loss: 1.4819 - acc: 0.4161 - val_loss: 1.3715 - val_acc: 0.3261\n",
      "Epoch 99/150\n",
      "6278/6278 [==============================] - 3s 409us/sample - loss: 1.4784 - acc: 0.4176 - val_loss: 1.2312 - val_acc: 0.3828\n",
      "Epoch 100/150\n",
      "6278/6278 [==============================] - 3s 412us/sample - loss: 1.4473 - acc: 0.4186 - val_loss: 1.4052 - val_acc: 0.3720\n",
      "Epoch 101/150\n",
      "6278/6278 [==============================] - 2s 398us/sample - loss: 1.4852 - acc: 0.4232 - val_loss: 1.3342 - val_acc: 0.2688\n",
      "Epoch 102/150\n",
      "6278/6278 [==============================] - 3s 417us/sample - loss: 1.4632 - acc: 0.4103 - val_loss: 1.3384 - val_acc: 0.3459\n",
      "Epoch 103/150\n",
      "6278/6278 [==============================] - 3s 413us/sample - loss: 1.4435 - acc: 0.4253 - val_loss: 1.2578 - val_acc: 0.3885\n",
      "Epoch 104/150\n",
      "6278/6278 [==============================] - 3s 429us/sample - loss: 1.4267 - acc: 0.4223 - val_loss: 1.3202 - val_acc: 0.3994\n",
      "Epoch 105/150\n",
      "6278/6278 [==============================] - 2s 397us/sample - loss: 1.4601 - acc: 0.4240 - val_loss: 1.3117 - val_acc: 0.3885\n",
      "Epoch 106/150\n",
      "6278/6278 [==============================] - 3s 408us/sample - loss: 1.4656 - acc: 0.4240 - val_loss: 1.2908 - val_acc: 0.3796\n",
      "Epoch 107/150\n",
      "6278/6278 [==============================] - 3s 405us/sample - loss: 1.4368 - acc: 0.4183 - val_loss: 1.3164 - val_acc: 0.2936\n",
      "Epoch 108/150\n",
      "6278/6278 [==============================] - 3s 419us/sample - loss: 1.4558 - acc: 0.4183 - val_loss: 1.3208 - val_acc: 0.3535\n",
      "Epoch 109/150\n",
      "6278/6278 [==============================] - 3s 399us/sample - loss: 1.4664 - acc: 0.4216 - val_loss: 1.3237 - val_acc: 0.3318\n",
      "Epoch 110/150\n",
      "6278/6278 [==============================] - 3s 425us/sample - loss: 1.4630 - acc: 0.4175 - val_loss: 1.3356 - val_acc: 0.3885\n",
      "Epoch 111/150\n",
      "6278/6278 [==============================] - 3s 405us/sample - loss: 1.4888 - acc: 0.4200 - val_loss: 1.2630 - val_acc: 0.3720\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6278/6278 [==============================] - 3s 414us/sample - loss: 1.4875 - acc: 0.4205 - val_loss: 1.2627 - val_acc: 0.3936\n",
      "Epoch 113/150\n",
      "6278/6278 [==============================] - 3s 400us/sample - loss: 1.4706 - acc: 0.4086 - val_loss: 1.2322 - val_acc: 0.4146\n",
      "Epoch 114/150\n",
      "6278/6278 [==============================] - 3s 411us/sample - loss: 1.4839 - acc: 0.4165 - val_loss: 1.2805 - val_acc: 0.4280\n",
      "Epoch 115/150\n",
      "6278/6278 [==============================] - 3s 408us/sample - loss: 1.4644 - acc: 0.4192 - val_loss: 1.2479 - val_acc: 0.3535\n",
      "Epoch 116/150\n",
      "6278/6278 [==============================] - 3s 408us/sample - loss: 1.4480 - acc: 0.4102 - val_loss: 1.2643 - val_acc: 0.4325\n",
      "Epoch 117/150\n",
      "6278/6278 [==============================] - 3s 409us/sample - loss: 1.4322 - acc: 0.4274 - val_loss: 1.2787 - val_acc: 0.2930\n",
      "Epoch 118/150\n",
      "6278/6278 [==============================] - 2s 397us/sample - loss: 1.4624 - acc: 0.4180 - val_loss: 1.2834 - val_acc: 0.3166\n",
      "Epoch 119/150\n",
      "6278/6278 [==============================] - 2s 398us/sample - loss: 1.4319 - acc: 0.4242 - val_loss: 1.2178 - val_acc: 0.3433\n",
      "Epoch 120/150\n",
      "6278/6278 [==============================] - 3s 407us/sample - loss: 1.4574 - acc: 0.4210 - val_loss: 1.2777 - val_acc: 0.3656\n",
      "Epoch 121/150\n",
      "6278/6278 [==============================] - 3s 417us/sample - loss: 1.3867 - acc: 0.4380 - val_loss: 1.2419 - val_acc: 0.3446\n",
      "Epoch 122/150\n",
      "6278/6278 [==============================] - 3s 412us/sample - loss: 1.4259 - acc: 0.4146 - val_loss: 1.3816 - val_acc: 0.3567\n",
      "Epoch 123/150\n",
      "6278/6278 [==============================] - 3s 426us/sample - loss: 1.4359 - acc: 0.4258 - val_loss: 1.2622 - val_acc: 0.4127\n",
      "Epoch 124/150\n",
      "6278/6278 [==============================] - 3s 400us/sample - loss: 1.4482 - acc: 0.4178 - val_loss: 1.2564 - val_acc: 0.3611\n",
      "Epoch 125/150\n",
      "6278/6278 [==============================] - 3s 423us/sample - loss: 1.4480 - acc: 0.4240 - val_loss: 1.2891 - val_acc: 0.3892\n",
      "Epoch 126/150\n",
      "6278/6278 [==============================] - 3s 402us/sample - loss: 1.4374 - acc: 0.4243 - val_loss: 1.2335 - val_acc: 0.3650\n",
      "Epoch 127/150\n",
      "6278/6278 [==============================] - 3s 399us/sample - loss: 1.4540 - acc: 0.4186 - val_loss: 1.3108 - val_acc: 0.3344\n",
      "Epoch 128/150\n",
      "6278/6278 [==============================] - 3s 412us/sample - loss: 1.4619 - acc: 0.4188 - val_loss: 1.2645 - val_acc: 0.4197\n",
      "Epoch 129/150\n",
      "6278/6278 [==============================] - 3s 416us/sample - loss: 1.4139 - acc: 0.4290 - val_loss: 1.3171 - val_acc: 0.3860\n",
      "Epoch 130/150\n",
      "6278/6278 [==============================] - 3s 404us/sample - loss: 1.4435 - acc: 0.4360 - val_loss: 1.2937 - val_acc: 0.3847\n",
      "Epoch 131/150\n",
      "6278/6278 [==============================] - 3s 419us/sample - loss: 1.4853 - acc: 0.4202 - val_loss: 1.2487 - val_acc: 0.4369\n",
      "Epoch 132/150\n",
      "6278/6278 [==============================] - 3s 403us/sample - loss: 1.4342 - acc: 0.4191 - val_loss: 1.1940 - val_acc: 0.3847\n",
      "Epoch 133/150\n",
      "6278/6278 [==============================] - 3s 418us/sample - loss: 1.4173 - acc: 0.4232 - val_loss: 1.3029 - val_acc: 0.3274\n",
      "Epoch 134/150\n",
      "6278/6278 [==============================] - 3s 414us/sample - loss: 1.4352 - acc: 0.4239 - val_loss: 1.2217 - val_acc: 0.3656\n",
      "Epoch 135/150\n",
      "6278/6278 [==============================] - 3s 428us/sample - loss: 1.4098 - acc: 0.4170 - val_loss: 1.3266 - val_acc: 0.3592\n",
      "Epoch 136/150\n",
      "6278/6278 [==============================] - 3s 403us/sample - loss: 1.4051 - acc: 0.4239 - val_loss: 1.2989 - val_acc: 0.3548\n",
      "Epoch 137/150\n",
      "6278/6278 [==============================] - 3s 428us/sample - loss: 1.4321 - acc: 0.4224 - val_loss: 1.2764 - val_acc: 0.3675\n",
      "Epoch 138/150\n",
      "6278/6278 [==============================] - 3s 404us/sample - loss: 1.4753 - acc: 0.4154 - val_loss: 1.2477 - val_acc: 0.3306\n",
      "Epoch 139/150\n",
      "6278/6278 [==============================] - 3s 426us/sample - loss: 1.4274 - acc: 0.4208 - val_loss: 1.2352 - val_acc: 0.3732\n",
      "Epoch 140/150\n",
      "6278/6278 [==============================] - 3s 417us/sample - loss: 1.4599 - acc: 0.4118 - val_loss: 1.2335 - val_acc: 0.3586\n",
      "Epoch 141/150\n",
      "6278/6278 [==============================] - 3s 438us/sample - loss: 1.4314 - acc: 0.4186 - val_loss: 1.1997 - val_acc: 0.3592\n",
      "Epoch 142/150\n",
      "6278/6278 [==============================] - 3s 409us/sample - loss: 1.4569 - acc: 0.4126 - val_loss: 1.2759 - val_acc: 0.3809\n",
      "Epoch 143/150\n",
      "6278/6278 [==============================] - 3s 442us/sample - loss: 1.4357 - acc: 0.4234 - val_loss: 1.3135 - val_acc: 0.3478\n",
      "Epoch 144/150\n",
      "6278/6278 [==============================] - 3s 418us/sample - loss: 1.4010 - acc: 0.4350 - val_loss: 1.3125 - val_acc: 0.4102\n",
      "Epoch 145/150\n",
      "6278/6278 [==============================] - 3s 444us/sample - loss: 1.4335 - acc: 0.4356 - val_loss: 1.2614 - val_acc: 0.3478\n",
      "Epoch 146/150\n",
      "6278/6278 [==============================] - 3s 403us/sample - loss: 1.4189 - acc: 0.4384 - val_loss: 1.2058 - val_acc: 0.3605\n",
      "Epoch 147/150\n",
      "6278/6278 [==============================] - 3s 428us/sample - loss: 1.4152 - acc: 0.4312 - val_loss: 1.2604 - val_acc: 0.3045\n",
      "Epoch 148/150\n",
      "6278/6278 [==============================] - 3s 419us/sample - loss: 1.4703 - acc: 0.4073 - val_loss: 1.2665 - val_acc: 0.3847\n",
      "Epoch 149/150\n",
      "6278/6278 [==============================] - 3s 430us/sample - loss: 1.4369 - acc: 0.4251 - val_loss: 1.2303 - val_acc: 0.3713\n",
      "Epoch 150/150\n",
      "6278/6278 [==============================] - 3s 409us/sample - loss: 1.4095 - acc: 0.4376 - val_loss: 1.2615 - val_acc: 0.3344\n",
      "6278\n",
      "1570\n",
      "Train on 6278 samples, validate on 1570 samples\n",
      "Epoch 1/150\n",
      "6278/6278 [==============================] - 3s 549us/sample - loss: 2.9556 - acc: 0.1354 - val_loss: 2.4465 - val_acc: 0.1089\n",
      "Epoch 2/150\n",
      "6278/6278 [==============================] - 3s 437us/sample - loss: 2.4839 - acc: 0.1862 - val_loss: 2.1074 - val_acc: 0.2713\n",
      "Epoch 3/150\n",
      "6278/6278 [==============================] - 3s 420us/sample - loss: 2.3827 - acc: 0.2064 - val_loss: 2.1392 - val_acc: 0.2529\n",
      "Epoch 4/150\n",
      "6278/6278 [==============================] - 3s 427us/sample - loss: 2.2308 - acc: 0.2418 - val_loss: 1.9579 - val_acc: 0.2510\n",
      "Epoch 5/150\n",
      "6278/6278 [==============================] - 3s 410us/sample - loss: 2.0962 - acc: 0.2560 - val_loss: 1.9950 - val_acc: 0.2841\n",
      "Epoch 6/150\n",
      "6278/6278 [==============================] - 3s 423us/sample - loss: 2.0374 - acc: 0.2687 - val_loss: 1.7571 - val_acc: 0.3134\n",
      "Epoch 7/150\n",
      "6278/6278 [==============================] - 3s 413us/sample - loss: 1.9534 - acc: 0.2824 - val_loss: 1.8004 - val_acc: 0.2834\n",
      "Epoch 8/150\n",
      "6278/6278 [==============================] - 3s 429us/sample - loss: 1.9480 - acc: 0.3052 - val_loss: 1.8091 - val_acc: 0.2771\n",
      "Epoch 9/150\n",
      "6278/6278 [==============================] - 3s 411us/sample - loss: 1.9003 - acc: 0.3101 - val_loss: 1.7809 - val_acc: 0.3076\n",
      "Epoch 10/150\n",
      "6278/6278 [==============================] - 3s 426us/sample - loss: 1.8615 - acc: 0.3221 - val_loss: 1.7635 - val_acc: 0.3236\n",
      "Epoch 11/150\n",
      "6278/6278 [==============================] - 3s 407us/sample - loss: 1.8209 - acc: 0.3269 - val_loss: 1.7950 - val_acc: 0.3102\n",
      "Epoch 12/150\n",
      "6278/6278 [==============================] - 3s 430us/sample - loss: 1.7918 - acc: 0.3218 - val_loss: 1.6799 - val_acc: 0.3611\n",
      "Epoch 13/150\n",
      "6278/6278 [==============================] - 3s 424us/sample - loss: 1.7541 - acc: 0.3391 - val_loss: 1.6628 - val_acc: 0.3548\n",
      "Epoch 14/150\n",
      "6278/6278 [==============================] - 3s 436us/sample - loss: 1.7995 - acc: 0.3291 - val_loss: 1.6343 - val_acc: 0.3299\n",
      "Epoch 15/150\n",
      "6278/6278 [==============================] - 3s 423us/sample - loss: 1.7138 - acc: 0.3484 - val_loss: 1.5836 - val_acc: 0.3682\n",
      "Epoch 16/150\n",
      "6278/6278 [==============================] - 3s 431us/sample - loss: 1.7398 - acc: 0.3480 - val_loss: 1.6539 - val_acc: 0.3497\n",
      "Epoch 17/150\n",
      "6278/6278 [==============================] - 3s 424us/sample - loss: 1.7105 - acc: 0.3402 - val_loss: 1.5980 - val_acc: 0.3669\n",
      "Epoch 18/150\n",
      "6278/6278 [==============================] - 3s 429us/sample - loss: 1.7607 - acc: 0.3503 - val_loss: 1.6343 - val_acc: 0.3592\n",
      "Epoch 19/150\n",
      "6278/6278 [==============================] - 3s 420us/sample - loss: 1.7261 - acc: 0.3496 - val_loss: 1.5570 - val_acc: 0.4522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/150\n",
      "6278/6278 [==============================] - 3s 433us/sample - loss: 1.6919 - acc: 0.3484 - val_loss: 1.6910 - val_acc: 0.3885\n",
      "Epoch 21/150\n",
      "6278/6278 [==============================] - 3s 415us/sample - loss: 1.6758 - acc: 0.3576 - val_loss: 1.6085 - val_acc: 0.3318\n",
      "Epoch 22/150\n",
      "6278/6278 [==============================] - 3s 436us/sample - loss: 1.6554 - acc: 0.3514 - val_loss: 1.6353 - val_acc: 0.3694\n",
      "Epoch 23/150\n",
      "6278/6278 [==============================] - 3s 415us/sample - loss: 1.6601 - acc: 0.3573 - val_loss: 1.5838 - val_acc: 0.3357\n",
      "Epoch 24/150\n",
      "6278/6278 [==============================] - 3s 431us/sample - loss: 1.6411 - acc: 0.3707 - val_loss: 1.5525 - val_acc: 0.3917\n",
      "Epoch 25/150\n",
      "6278/6278 [==============================] - 3s 408us/sample - loss: 1.6368 - acc: 0.3675 - val_loss: 1.5297 - val_acc: 0.4376\n",
      "Epoch 26/150\n",
      "6278/6278 [==============================] - 3s 428us/sample - loss: 1.6438 - acc: 0.3665 - val_loss: 1.5373 - val_acc: 0.4038\n",
      "Epoch 27/150\n",
      "6278/6278 [==============================] - 3s 419us/sample - loss: 1.6363 - acc: 0.3659 - val_loss: 1.5410 - val_acc: 0.3930\n",
      "Epoch 28/150\n",
      "6278/6278 [==============================] - 3s 434us/sample - loss: 1.5857 - acc: 0.3756 - val_loss: 1.4975 - val_acc: 0.3752\n",
      "Epoch 29/150\n",
      "6278/6278 [==============================] - 3s 415us/sample - loss: 1.6052 - acc: 0.3681 - val_loss: 1.5193 - val_acc: 0.4446\n",
      "Epoch 30/150\n",
      "6278/6278 [==============================] - 3s 443us/sample - loss: 1.6197 - acc: 0.3673 - val_loss: 1.5147 - val_acc: 0.3936\n",
      "Epoch 31/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.5979 - acc: 0.3691 - val_loss: 1.5372 - val_acc: 0.3764\n",
      "Epoch 32/150\n",
      "6278/6278 [==============================] - 3s 444us/sample - loss: 1.5986 - acc: 0.3724 - val_loss: 1.5271 - val_acc: 0.3656\n",
      "Epoch 33/150\n",
      "6278/6278 [==============================] - 3s 436us/sample - loss: 1.6035 - acc: 0.3681 - val_loss: 1.5582 - val_acc: 0.3662\n",
      "Epoch 34/150\n",
      "6278/6278 [==============================] - 3s 415us/sample - loss: 1.6049 - acc: 0.3715 - val_loss: 1.5069 - val_acc: 0.3904\n",
      "Epoch 35/150\n",
      "6278/6278 [==============================] - 3s 412us/sample - loss: 1.5748 - acc: 0.3727 - val_loss: 1.5527 - val_acc: 0.3306\n",
      "Epoch 36/150\n",
      "6278/6278 [==============================] - 3s 452us/sample - loss: 1.5457 - acc: 0.3734 - val_loss: 1.4677 - val_acc: 0.4064\n",
      "Epoch 37/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.6101 - acc: 0.3723 - val_loss: 1.5553 - val_acc: 0.3669\n",
      "Epoch 38/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.5807 - acc: 0.3735 - val_loss: 1.4328 - val_acc: 0.4146\n",
      "Epoch 39/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.5532 - acc: 0.3761 - val_loss: 1.3988 - val_acc: 0.4121\n",
      "Epoch 40/150\n",
      "6278/6278 [==============================] - 2s 398us/sample - loss: 1.5560 - acc: 0.3847 - val_loss: 1.5169 - val_acc: 0.3471\n",
      "Epoch 41/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.5470 - acc: 0.3689 - val_loss: 1.4979 - val_acc: 0.4611\n",
      "Epoch 42/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.5559 - acc: 0.3794 - val_loss: 1.4238 - val_acc: 0.3605\n",
      "Epoch 43/150\n",
      "6278/6278 [==============================] - 3s 406us/sample - loss: 1.5307 - acc: 0.3823 - val_loss: 1.4571 - val_acc: 0.3739\n",
      "Epoch 44/150\n",
      "6278/6278 [==============================] - 3s 399us/sample - loss: 1.5223 - acc: 0.3928 - val_loss: 1.4525 - val_acc: 0.4089\n",
      "Epoch 45/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.5428 - acc: 0.3917 - val_loss: 1.4548 - val_acc: 0.4261\n",
      "Epoch 46/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.5266 - acc: 0.3809 - val_loss: 1.4496 - val_acc: 0.3841\n",
      "Epoch 47/150\n",
      "6278/6278 [==============================] - 3s 408us/sample - loss: 1.5281 - acc: 0.3796 - val_loss: 1.4405 - val_acc: 0.4236\n",
      "Epoch 48/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.5093 - acc: 0.3856 - val_loss: 1.5093 - val_acc: 0.3484\n",
      "Epoch 49/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.5385 - acc: 0.3852 - val_loss: 1.4933 - val_acc: 0.3777\n",
      "Epoch 50/150\n",
      "6278/6278 [==============================] - 2s 395us/sample - loss: 1.5221 - acc: 0.3742 - val_loss: 1.3994 - val_acc: 0.4166\n",
      "Epoch 51/150\n",
      "6278/6278 [==============================] - 3s 405us/sample - loss: 1.5031 - acc: 0.3831 - val_loss: 1.4948 - val_acc: 0.4025\n",
      "Epoch 52/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.5225 - acc: 0.3848 - val_loss: 1.3952 - val_acc: 0.4554\n",
      "Epoch 53/150\n",
      "6278/6278 [==============================] - 2s 397us/sample - loss: 1.4925 - acc: 0.3852 - val_loss: 1.4370 - val_acc: 0.3694\n",
      "Epoch 54/150\n",
      "6278/6278 [==============================] - 2s 398us/sample - loss: 1.4963 - acc: 0.3910 - val_loss: 1.4895 - val_acc: 0.3490\n",
      "Epoch 55/150\n",
      "6278/6278 [==============================] - 3s 398us/sample - loss: 1.5282 - acc: 0.3799 - val_loss: 1.4573 - val_acc: 0.3726\n",
      "Epoch 56/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.5375 - acc: 0.3743 - val_loss: 1.4445 - val_acc: 0.3796\n",
      "Epoch 57/150\n",
      "6278/6278 [==============================] - ETA: 0s - loss: 1.5241 - acc: 0.385 - 2s 385us/sample - loss: 1.5263 - acc: 0.3858 - val_loss: 1.4758 - val_acc: 0.4076\n",
      "Epoch 58/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.5287 - acc: 0.3903 - val_loss: 1.6118 - val_acc: 0.3420\n",
      "Epoch 59/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.4854 - acc: 0.3877 - val_loss: 1.4665 - val_acc: 0.4478\n",
      "Epoch 60/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.5339 - acc: 0.3914 - val_loss: 1.4630 - val_acc: 0.4006\n",
      "Epoch 61/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.4722 - acc: 0.3893 - val_loss: 1.4684 - val_acc: 0.3987\n",
      "Epoch 62/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4688 - acc: 0.3895 - val_loss: 1.4278 - val_acc: 0.3879\n",
      "Epoch 63/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.4738 - acc: 0.3887 - val_loss: 1.4587 - val_acc: 0.3898\n",
      "Epoch 64/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.4753 - acc: 0.3874 - val_loss: 1.4675 - val_acc: 0.4318\n",
      "Epoch 65/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4857 - acc: 0.3882 - val_loss: 1.4462 - val_acc: 0.4166\n",
      "Epoch 66/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.4850 - acc: 0.3944 - val_loss: 1.4360 - val_acc: 0.4076\n",
      "Epoch 67/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.4582 - acc: 0.3952 - val_loss: 1.4768 - val_acc: 0.3726\n",
      "Epoch 68/150\n",
      "6278/6278 [==============================] - 2s 395us/sample - loss: 1.4711 - acc: 0.3869 - val_loss: 1.4540 - val_acc: 0.3917\n",
      "Epoch 69/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4681 - acc: 0.3918 - val_loss: 1.4003 - val_acc: 0.4414\n",
      "Epoch 70/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.4757 - acc: 0.3890 - val_loss: 1.4507 - val_acc: 0.3994\n",
      "Epoch 71/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.4747 - acc: 0.3858 - val_loss: 1.4646 - val_acc: 0.3688\n",
      "Epoch 72/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.4508 - acc: 0.4067 - val_loss: 1.4298 - val_acc: 0.4019\n",
      "Epoch 73/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.4675 - acc: 0.3812 - val_loss: 1.4486 - val_acc: 0.4210\n",
      "Epoch 74/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4837 - acc: 0.4138 - val_loss: 1.4443 - val_acc: 0.4070\n",
      "Epoch 75/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.4674 - acc: 0.3893 - val_loss: 1.4800 - val_acc: 0.3955\n",
      "Epoch 76/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.4236 - acc: 0.3982 - val_loss: 1.4140 - val_acc: 0.3656\n",
      "Epoch 77/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.4763 - acc: 0.3872 - val_loss: 1.4925 - val_acc: 0.3669\n",
      "Epoch 78/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.5193 - acc: 0.3809 - val_loss: 1.4794 - val_acc: 0.3924\n",
      "Epoch 79/150\n",
      "6278/6278 [==============================] - 2s 395us/sample - loss: 1.4526 - acc: 0.4051 - val_loss: 1.4580 - val_acc: 0.3930\n",
      "Epoch 80/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.4574 - acc: 0.3993 - val_loss: 1.3993 - val_acc: 0.4191\n",
      "Epoch 81/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4505 - acc: 0.3968 - val_loss: 1.4751 - val_acc: 0.3701\n",
      "Epoch 82/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4346 - acc: 0.3938 - val_loss: 1.3999 - val_acc: 0.4459\n",
      "Epoch 83/150\n",
      "6278/6278 [==============================] - 2s 397us/sample - loss: 1.4232 - acc: 0.4079 - val_loss: 1.3827 - val_acc: 0.4446\n",
      "Epoch 84/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.4506 - acc: 0.4011 - val_loss: 1.4239 - val_acc: 0.4204\n",
      "Epoch 85/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4298 - acc: 0.4032 - val_loss: 1.4386 - val_acc: 0.3682\n",
      "Epoch 86/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4801 - acc: 0.3969 - val_loss: 1.4597 - val_acc: 0.3522\n",
      "Epoch 87/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.4176 - acc: 0.4014 - val_loss: 1.4163 - val_acc: 0.4414\n",
      "Epoch 88/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.4338 - acc: 0.4057 - val_loss: 1.4763 - val_acc: 0.3548\n",
      "Epoch 89/150\n",
      "6278/6278 [==============================] - 2s 382us/sample - loss: 1.4633 - acc: 0.3942 - val_loss: 1.4796 - val_acc: 0.4159\n",
      "Epoch 90/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.4214 - acc: 0.3981 - val_loss: 1.4336 - val_acc: 0.4083\n",
      "Epoch 91/150\n",
      "6278/6278 [==============================] - 2s 377us/sample - loss: 1.4379 - acc: 0.4041 - val_loss: 1.4242 - val_acc: 0.3841\n",
      "Epoch 92/150\n",
      "6278/6278 [==============================] - 2s 397us/sample - loss: 1.4466 - acc: 0.3939 - val_loss: 1.4618 - val_acc: 0.4395\n",
      "Epoch 93/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.4271 - acc: 0.4178 - val_loss: 1.3970 - val_acc: 0.3752\n",
      "Epoch 94/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.3982 - acc: 0.4156 - val_loss: 1.4389 - val_acc: 0.3962\n",
      "Epoch 95/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.4574 - acc: 0.3947 - val_loss: 1.4286 - val_acc: 0.4274\n",
      "Epoch 96/150\n",
      "6278/6278 [==============================] - 2s 393us/sample - loss: 1.4280 - acc: 0.4038 - val_loss: 1.4172 - val_acc: 0.4064\n",
      "Epoch 97/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4581 - acc: 0.4038 - val_loss: 1.4104 - val_acc: 0.3866\n",
      "Epoch 98/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.4708 - acc: 0.3996 - val_loss: 1.4591 - val_acc: 0.3854\n",
      "Epoch 99/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4528 - acc: 0.4089 - val_loss: 1.3910 - val_acc: 0.4580\n",
      "Epoch 100/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.4341 - acc: 0.4004 - val_loss: 1.4003 - val_acc: 0.4268\n",
      "Epoch 101/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.4339 - acc: 0.3958 - val_loss: 1.4113 - val_acc: 0.4006\n",
      "Epoch 102/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4377 - acc: 0.4116 - val_loss: 1.3951 - val_acc: 0.4790\n",
      "Epoch 103/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4198 - acc: 0.4063 - val_loss: 1.3930 - val_acc: 0.4350\n",
      "Epoch 104/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.4269 - acc: 0.4081 - val_loss: 1.4034 - val_acc: 0.4236\n",
      "Epoch 105/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.4407 - acc: 0.4071 - val_loss: 1.5721 - val_acc: 0.3618\n",
      "Epoch 106/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4070 - acc: 0.4017 - val_loss: 1.4062 - val_acc: 0.3873\n",
      "Epoch 107/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.4200 - acc: 0.3996 - val_loss: 1.4057 - val_acc: 0.4618\n",
      "Epoch 108/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.4046 - acc: 0.4110 - val_loss: 1.4078 - val_acc: 0.3930\n",
      "Epoch 109/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.4225 - acc: 0.4054 - val_loss: 1.4667 - val_acc: 0.3707\n",
      "Epoch 110/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.3789 - acc: 0.4076 - val_loss: 1.4278 - val_acc: 0.3879\n",
      "Epoch 111/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4155 - acc: 0.4237 - val_loss: 1.4370 - val_acc: 0.3783\n",
      "Epoch 112/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.4188 - acc: 0.4057 - val_loss: 1.4703 - val_acc: 0.3866\n",
      "Epoch 113/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.3929 - acc: 0.4172 - val_loss: 1.4405 - val_acc: 0.4000\n",
      "Epoch 114/150\n",
      "6278/6278 [==============================] - 2s 395us/sample - loss: 1.4004 - acc: 0.4033 - val_loss: 1.4307 - val_acc: 0.4153\n",
      "Epoch 115/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.3898 - acc: 0.4017 - val_loss: 1.4267 - val_acc: 0.4541\n",
      "Epoch 116/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.3624 - acc: 0.4205 - val_loss: 1.3846 - val_acc: 0.4471\n",
      "Epoch 117/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.4402 - acc: 0.4052 - val_loss: 1.4250 - val_acc: 0.4064\n",
      "Epoch 118/150\n",
      "6278/6278 [==============================] - 2s 393us/sample - loss: 1.4001 - acc: 0.4133 - val_loss: 1.4132 - val_acc: 0.4076\n",
      "Epoch 119/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.3991 - acc: 0.4302 - val_loss: 1.4758 - val_acc: 0.3834\n",
      "Epoch 120/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.3859 - acc: 0.4083 - val_loss: 1.4598 - val_acc: 0.4178\n",
      "Epoch 121/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4132 - acc: 0.4196 - val_loss: 1.4464 - val_acc: 0.3796\n",
      "Epoch 122/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.3849 - acc: 0.4110 - val_loss: 1.4546 - val_acc: 0.4414\n",
      "Epoch 123/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.4249 - acc: 0.4047 - val_loss: 1.4581 - val_acc: 0.4236\n",
      "Epoch 124/150\n",
      "6278/6278 [==============================] - 2s 395us/sample - loss: 1.4398 - acc: 0.4172 - val_loss: 1.4098 - val_acc: 0.3854\n",
      "Epoch 125/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.3868 - acc: 0.4132 - val_loss: 1.4107 - val_acc: 0.3860\n",
      "Epoch 126/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4042 - acc: 0.4095 - val_loss: 1.4433 - val_acc: 0.3726\n",
      "Epoch 127/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.3818 - acc: 0.4192 - val_loss: 1.4076 - val_acc: 0.3943\n",
      "Epoch 128/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4198 - acc: 0.4118 - val_loss: 1.4552 - val_acc: 0.3955\n",
      "Epoch 129/150\n",
      "6278/6278 [==============================] - 2s 397us/sample - loss: 1.4212 - acc: 0.4084 - val_loss: 1.4332 - val_acc: 0.4338\n",
      "Epoch 130/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.3843 - acc: 0.4176 - val_loss: 1.4258 - val_acc: 0.4236\n",
      "Epoch 131/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.3454 - acc: 0.4116 - val_loss: 1.4240 - val_acc: 0.3975\n",
      "Epoch 132/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.4099 - acc: 0.4019 - val_loss: 1.3969 - val_acc: 0.4159\n",
      "Epoch 133/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.3757 - acc: 0.4180 - val_loss: 1.5041 - val_acc: 0.3981\n",
      "Epoch 134/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4120 - acc: 0.4020 - val_loss: 1.4288 - val_acc: 0.3873\n",
      "Epoch 135/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.3606 - acc: 0.4202 - val_loss: 1.3643 - val_acc: 0.4350\n",
      "Epoch 136/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4115 - acc: 0.4057 - val_loss: 1.3563 - val_acc: 0.4548\n",
      "Epoch 137/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.3872 - acc: 0.4017 - val_loss: 1.3775 - val_acc: 0.4191\n",
      "Epoch 138/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.3811 - acc: 0.4003 - val_loss: 1.4271 - val_acc: 0.4299\n",
      "Epoch 139/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.3629 - acc: 0.4216 - val_loss: 1.3585 - val_acc: 0.3745\n",
      "Epoch 140/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.3871 - acc: 0.4084 - val_loss: 1.4401 - val_acc: 0.3637\n",
      "Epoch 141/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.4174 - acc: 0.4011 - val_loss: 1.4289 - val_acc: 0.3790\n",
      "Epoch 142/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.3653 - acc: 0.4097 - val_loss: 1.3767 - val_acc: 0.4376\n",
      "Epoch 143/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.3854 - acc: 0.4159 - val_loss: 1.4110 - val_acc: 0.3936\n",
      "Epoch 144/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.3746 - acc: 0.4140 - val_loss: 1.3880 - val_acc: 0.4172\n",
      "Epoch 145/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.3685 - acc: 0.4140 - val_loss: 1.3400 - val_acc: 0.4312\n",
      "Epoch 146/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.3564 - acc: 0.4251 - val_loss: 1.3941 - val_acc: 0.4191\n",
      "Epoch 147/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.3944 - acc: 0.4162 - val_loss: 1.4318 - val_acc: 0.4140\n",
      "Epoch 148/150\n",
      "6278/6278 [==============================] - 3s 401us/sample - loss: 1.3578 - acc: 0.4153 - val_loss: 1.3970 - val_acc: 0.3904\n",
      "Epoch 149/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.3671 - acc: 0.4210 - val_loss: 1.3800 - val_acc: 0.3764\n",
      "Epoch 150/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.3667 - acc: 0.4164 - val_loss: 1.4707 - val_acc: 0.3860\n",
      "6278\n",
      "1570\n",
      "Train on 6278 samples, validate on 1570 samples\n",
      "Epoch 1/150\n",
      "6278/6278 [==============================] - 3s 503us/sample - loss: 2.9230 - acc: 0.1301 - val_loss: 2.4911 - val_acc: 0.2127\n",
      "Epoch 2/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 2.5071 - acc: 0.1717 - val_loss: 2.1215 - val_acc: 0.2586\n",
      "Epoch 3/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 2.3757 - acc: 0.2005 - val_loss: 2.0674 - val_acc: 0.3127\n",
      "Epoch 4/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 2.1980 - acc: 0.2278 - val_loss: 1.9980 - val_acc: 0.2783\n",
      "Epoch 5/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 2.1206 - acc: 0.2633 - val_loss: 2.0399 - val_acc: 0.2293\n",
      "Epoch 6/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 2.0524 - acc: 0.2694 - val_loss: 2.1068 - val_acc: 0.3064\n",
      "Epoch 7/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.9898 - acc: 0.2845 - val_loss: 1.8153 - val_acc: 0.3013\n",
      "Epoch 8/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.9343 - acc: 0.2971 - val_loss: 1.8623 - val_acc: 0.2790\n",
      "Epoch 9/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.8979 - acc: 0.3068 - val_loss: 1.7870 - val_acc: 0.3510\n",
      "Epoch 10/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.8449 - acc: 0.3100 - val_loss: 1.7150 - val_acc: 0.3439\n",
      "Epoch 11/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.8653 - acc: 0.3198 - val_loss: 1.6837 - val_acc: 0.3815\n",
      "Epoch 12/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.7782 - acc: 0.3256 - val_loss: 1.6111 - val_acc: 0.4217\n",
      "Epoch 13/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.7517 - acc: 0.3264 - val_loss: 1.6943 - val_acc: 0.3389\n",
      "Epoch 14/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.7349 - acc: 0.3281 - val_loss: 1.6696 - val_acc: 0.3745\n",
      "Epoch 15/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.7563 - acc: 0.3410 - val_loss: 1.6429 - val_acc: 0.4503\n",
      "Epoch 16/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.7584 - acc: 0.3345 - val_loss: 1.7579 - val_acc: 0.3318\n",
      "Epoch 17/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.7083 - acc: 0.3386 - val_loss: 1.8004 - val_acc: 0.3153\n",
      "Epoch 18/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.7306 - acc: 0.3348 - val_loss: 1.7149 - val_acc: 0.3503\n",
      "Epoch 19/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.6945 - acc: 0.3544 - val_loss: 1.5672 - val_acc: 0.4153\n",
      "Epoch 20/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.6951 - acc: 0.3471 - val_loss: 1.5636 - val_acc: 0.3580\n",
      "Epoch 21/150\n",
      "6278/6278 [==============================] - 2s 393us/sample - loss: 1.6706 - acc: 0.3519 - val_loss: 1.7917 - val_acc: 0.3777\n",
      "Epoch 22/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.6785 - acc: 0.3630 - val_loss: 1.6057 - val_acc: 0.3860\n",
      "Epoch 23/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.6441 - acc: 0.3479 - val_loss: 1.5344 - val_acc: 0.3599\n",
      "Epoch 24/150\n",
      "6278/6278 [==============================] - 2s 382us/sample - loss: 1.6298 - acc: 0.3547 - val_loss: 1.6506 - val_acc: 0.3739\n",
      "Epoch 25/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.6788 - acc: 0.3530 - val_loss: 1.6442 - val_acc: 0.3541\n",
      "Epoch 26/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.6497 - acc: 0.3503 - val_loss: 1.5290 - val_acc: 0.3803\n",
      "Epoch 27/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.6036 - acc: 0.3598 - val_loss: 1.5652 - val_acc: 0.3764\n",
      "Epoch 28/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.6030 - acc: 0.3635 - val_loss: 1.5744 - val_acc: 0.3567\n",
      "Epoch 29/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.6192 - acc: 0.3633 - val_loss: 1.7644 - val_acc: 0.3892\n",
      "Epoch 30/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.6008 - acc: 0.3493 - val_loss: 1.4995 - val_acc: 0.4121\n",
      "Epoch 31/150\n",
      "6278/6278 [==============================] - 2s 382us/sample - loss: 1.5908 - acc: 0.3686 - val_loss: 1.5306 - val_acc: 0.3675\n",
      "Epoch 32/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.6256 - acc: 0.3662 - val_loss: 1.4682 - val_acc: 0.4025\n",
      "Epoch 33/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.5517 - acc: 0.3812 - val_loss: 1.5010 - val_acc: 0.3752\n",
      "Epoch 34/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.5705 - acc: 0.3637 - val_loss: 1.4815 - val_acc: 0.4025\n",
      "Epoch 35/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.5994 - acc: 0.3644 - val_loss: 1.5170 - val_acc: 0.3860\n",
      "Epoch 36/150\n",
      "6278/6278 [==============================] - 2s 395us/sample - loss: 1.5759 - acc: 0.3716 - val_loss: 1.5877 - val_acc: 0.4204\n",
      "Epoch 37/150\n",
      "6278/6278 [==============================] - 2s 382us/sample - loss: 1.5379 - acc: 0.3719 - val_loss: 1.5125 - val_acc: 0.4038\n",
      "Epoch 38/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.6001 - acc: 0.3595 - val_loss: 1.4928 - val_acc: 0.3739\n",
      "Epoch 39/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.5463 - acc: 0.3637 - val_loss: 1.4942 - val_acc: 0.3828\n",
      "Epoch 40/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.5503 - acc: 0.3683 - val_loss: 1.4885 - val_acc: 0.4331\n",
      "Epoch 41/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.5766 - acc: 0.3595 - val_loss: 1.5217 - val_acc: 0.4618\n",
      "Epoch 42/150\n",
      "6278/6278 [==============================] - 2s 382us/sample - loss: 1.5590 - acc: 0.3695 - val_loss: 1.5338 - val_acc: 0.3401\n",
      "Epoch 43/150\n",
      "6278/6278 [==============================] - 2s 395us/sample - loss: 1.5333 - acc: 0.3662 - val_loss: 1.4671 - val_acc: 0.4306\n",
      "Epoch 44/150\n",
      "6278/6278 [==============================] - 3s 415us/sample - loss: 1.5665 - acc: 0.3684 - val_loss: 1.5199 - val_acc: 0.3713\n",
      "Epoch 45/150\n",
      "6278/6278 [==============================] - 3s 430us/sample - loss: 1.5412 - acc: 0.3785 - val_loss: 1.4728 - val_acc: 0.3955\n",
      "Epoch 46/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.5021 - acc: 0.3866 - val_loss: 1.4379 - val_acc: 0.3949\n",
      "Epoch 47/150\n",
      "6278/6278 [==============================] - 3s 400us/sample - loss: 1.5075 - acc: 0.3805 - val_loss: 1.4677 - val_acc: 0.4121\n",
      "Epoch 48/150\n",
      "6278/6278 [==============================] - 2s 377us/sample - loss: 1.5172 - acc: 0.3836 - val_loss: 1.4627 - val_acc: 0.3955\n",
      "Epoch 49/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.5085 - acc: 0.3775 - val_loss: 1.4676 - val_acc: 0.4185\n",
      "Epoch 50/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.5041 - acc: 0.3852 - val_loss: 1.4624 - val_acc: 0.3713\n",
      "Epoch 51/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.5241 - acc: 0.3691 - val_loss: 1.4892 - val_acc: 0.4344\n",
      "Epoch 52/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.5506 - acc: 0.3888 - val_loss: 1.5990 - val_acc: 0.3879\n",
      "Epoch 53/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.4953 - acc: 0.3759 - val_loss: 1.4704 - val_acc: 0.4076\n",
      "Epoch 54/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.5158 - acc: 0.3874 - val_loss: 1.4966 - val_acc: 0.3809\n",
      "Epoch 55/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.5225 - acc: 0.3864 - val_loss: 1.4819 - val_acc: 0.3898\n",
      "Epoch 56/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.4897 - acc: 0.3828 - val_loss: 1.4645 - val_acc: 0.3847\n",
      "Epoch 57/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.4912 - acc: 0.3893 - val_loss: 1.4030 - val_acc: 0.3968\n",
      "Epoch 58/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.5102 - acc: 0.3837 - val_loss: 1.4521 - val_acc: 0.4414\n",
      "Epoch 59/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.4755 - acc: 0.4006 - val_loss: 1.4788 - val_acc: 0.3936\n",
      "Epoch 60/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.4622 - acc: 0.3877 - val_loss: 1.4500 - val_acc: 0.4121\n",
      "Epoch 61/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4571 - acc: 0.3938 - val_loss: 1.4972 - val_acc: 0.3885\n",
      "Epoch 62/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.4929 - acc: 0.3764 - val_loss: 1.4859 - val_acc: 0.3892\n",
      "Epoch 63/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.4733 - acc: 0.3899 - val_loss: 1.4712 - val_acc: 0.3936\n",
      "Epoch 64/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.5129 - acc: 0.3812 - val_loss: 1.5140 - val_acc: 0.3949\n",
      "Epoch 65/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.4662 - acc: 0.3906 - val_loss: 1.4481 - val_acc: 0.4000\n",
      "Epoch 66/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4695 - acc: 0.3922 - val_loss: 1.4879 - val_acc: 0.4274\n",
      "Epoch 67/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.4316 - acc: 0.3864 - val_loss: 1.4114 - val_acc: 0.4025\n",
      "Epoch 68/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.4651 - acc: 0.3961 - val_loss: 1.4890 - val_acc: 0.3701\n",
      "Epoch 69/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4799 - acc: 0.3836 - val_loss: 1.4647 - val_acc: 0.3994\n",
      "Epoch 70/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.4737 - acc: 0.3860 - val_loss: 1.4596 - val_acc: 0.4280\n",
      "Epoch 71/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.4653 - acc: 0.3855 - val_loss: 1.4539 - val_acc: 0.3911\n",
      "Epoch 72/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4430 - acc: 0.3928 - val_loss: 1.4920 - val_acc: 0.4089\n",
      "Epoch 73/150\n",
      "6278/6278 [==============================] - 3s 401us/sample - loss: 1.4760 - acc: 0.3961 - val_loss: 1.4250 - val_acc: 0.4197\n",
      "Epoch 74/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.4908 - acc: 0.3923 - val_loss: 1.5011 - val_acc: 0.4389\n",
      "Epoch 75/150\n",
      "6278/6278 [==============================] - 2s 397us/sample - loss: 1.4683 - acc: 0.4079 - val_loss: 1.4640 - val_acc: 0.3885\n",
      "Epoch 76/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.4465 - acc: 0.3866 - val_loss: 1.4387 - val_acc: 0.3834\n",
      "Epoch 77/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4416 - acc: 0.3989 - val_loss: 1.5156 - val_acc: 0.3745\n",
      "Epoch 78/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.4659 - acc: 0.3887 - val_loss: 1.4242 - val_acc: 0.3834\n",
      "Epoch 79/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.3998 - acc: 0.3942 - val_loss: 1.4195 - val_acc: 0.4318\n",
      "Epoch 80/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.4559 - acc: 0.3993 - val_loss: 1.4157 - val_acc: 0.4783\n",
      "Epoch 81/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4668 - acc: 0.3955 - val_loss: 1.3909 - val_acc: 0.4325\n",
      "Epoch 82/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4412 - acc: 0.3946 - val_loss: 1.4694 - val_acc: 0.3841\n",
      "Epoch 83/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4486 - acc: 0.3805 - val_loss: 1.4520 - val_acc: 0.3796\n",
      "Epoch 84/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.4197 - acc: 0.3874 - val_loss: 1.4016 - val_acc: 0.4032\n",
      "Epoch 85/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.4288 - acc: 0.3920 - val_loss: 1.4269 - val_acc: 0.4115\n",
      "Epoch 86/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4431 - acc: 0.3893 - val_loss: 1.4841 - val_acc: 0.4236\n",
      "Epoch 87/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4400 - acc: 0.3944 - val_loss: 1.4601 - val_acc: 0.3936\n",
      "Epoch 88/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4634 - acc: 0.3860 - val_loss: 1.4636 - val_acc: 0.3892\n",
      "Epoch 89/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.4170 - acc: 0.3844 - val_loss: 1.4901 - val_acc: 0.3860\n",
      "Epoch 90/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.3948 - acc: 0.3947 - val_loss: 1.4199 - val_acc: 0.4204\n",
      "Epoch 91/150\n",
      "6278/6278 [==============================] - 2s 382us/sample - loss: 1.4077 - acc: 0.4081 - val_loss: 1.4310 - val_acc: 0.4153\n",
      "Epoch 92/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4481 - acc: 0.3976 - val_loss: 1.4424 - val_acc: 0.4261\n",
      "Epoch 93/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.4660 - acc: 0.3836 - val_loss: 1.4571 - val_acc: 0.3924\n",
      "Epoch 94/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.4222 - acc: 0.3928 - val_loss: 1.3976 - val_acc: 0.3981\n",
      "Epoch 95/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4171 - acc: 0.3998 - val_loss: 1.3850 - val_acc: 0.4025\n",
      "Epoch 96/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.4183 - acc: 0.3933 - val_loss: 1.4468 - val_acc: 0.4127\n",
      "Epoch 97/150\n",
      "6278/6278 [==============================] - 2s 393us/sample - loss: 1.4227 - acc: 0.3969 - val_loss: 1.4431 - val_acc: 0.4045\n",
      "Epoch 98/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.4285 - acc: 0.3957 - val_loss: 1.4148 - val_acc: 0.4280\n",
      "Epoch 99/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.4011 - acc: 0.3969 - val_loss: 1.4302 - val_acc: 0.4490\n",
      "Epoch 100/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.4139 - acc: 0.3879 - val_loss: 1.4121 - val_acc: 0.3873\n",
      "Epoch 101/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.4028 - acc: 0.4030 - val_loss: 1.4510 - val_acc: 0.3898\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.3868 - acc: 0.3823 - val_loss: 1.4013 - val_acc: 0.3739\n",
      "Epoch 103/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.3921 - acc: 0.3990 - val_loss: 1.4314 - val_acc: 0.3752\n",
      "Epoch 104/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.4023 - acc: 0.4004 - val_loss: 1.4053 - val_acc: 0.4382\n",
      "Epoch 105/150\n",
      "6278/6278 [==============================] - 2s 396us/sample - loss: 1.4071 - acc: 0.3974 - val_loss: 1.4424 - val_acc: 0.3834\n",
      "Epoch 106/150\n",
      "6278/6278 [==============================] - 2s 374us/sample - loss: 1.4267 - acc: 0.3976 - val_loss: 1.4266 - val_acc: 0.4064\n",
      "Epoch 107/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.3972 - acc: 0.4040 - val_loss: 1.3750 - val_acc: 0.4401\n",
      "Epoch 108/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.3896 - acc: 0.4090 - val_loss: 1.4104 - val_acc: 0.4631\n",
      "Epoch 109/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.3705 - acc: 0.4044 - val_loss: 1.4849 - val_acc: 0.4554\n",
      "Epoch 110/150\n",
      "6278/6278 [==============================] - 2s 375us/sample - loss: 1.3921 - acc: 0.3982 - val_loss: 1.3538 - val_acc: 0.4357\n",
      "Epoch 111/150\n",
      "6278/6278 [==============================] - 2s 378us/sample - loss: 1.4306 - acc: 0.4006 - val_loss: 1.4820 - val_acc: 0.3873\n",
      "Epoch 112/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.3801 - acc: 0.3996 - val_loss: 1.5425 - val_acc: 0.3873\n",
      "Epoch 113/150\n",
      "6278/6278 [==============================] - 2s 380us/sample - loss: 1.3848 - acc: 0.3979 - val_loss: 1.3887 - val_acc: 0.4503\n",
      "Epoch 114/150\n",
      "6278/6278 [==============================] - 2s 391us/sample - loss: 1.4041 - acc: 0.4094 - val_loss: 1.4788 - val_acc: 0.4242\n",
      "Epoch 115/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4216 - acc: 0.3965 - val_loss: 1.4083 - val_acc: 0.4140\n",
      "Epoch 116/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.4251 - acc: 0.3953 - val_loss: 1.4170 - val_acc: 0.4401\n",
      "Epoch 117/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.3918 - acc: 0.3981 - val_loss: 1.3863 - val_acc: 0.4280\n",
      "Epoch 118/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.3928 - acc: 0.4071 - val_loss: 1.3925 - val_acc: 0.4318\n",
      "Epoch 119/150\n",
      "6278/6278 [==============================] - 2s 376us/sample - loss: 1.3897 - acc: 0.4009 - val_loss: 1.4281 - val_acc: 0.4299\n",
      "Epoch 120/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.4180 - acc: 0.3966 - val_loss: 1.4111 - val_acc: 0.4045\n",
      "Epoch 121/150\n",
      "6278/6278 [==============================] - 2s 388us/sample - loss: 1.4756 - acc: 0.3912 - val_loss: 1.4513 - val_acc: 0.4318\n",
      "Epoch 122/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.3672 - acc: 0.4038 - val_loss: 1.3979 - val_acc: 0.4089\n",
      "Epoch 123/150\n",
      "6278/6278 [==============================] - 2s 395us/sample - loss: 1.4088 - acc: 0.3985 - val_loss: 1.4069 - val_acc: 0.4006\n",
      "Epoch 124/150\n",
      "6278/6278 [==============================] - 2s 376us/sample - loss: 1.3799 - acc: 0.4067 - val_loss: 1.4127 - val_acc: 0.4134\n",
      "Epoch 125/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.3949 - acc: 0.4060 - val_loss: 1.4325 - val_acc: 0.4045\n",
      "Epoch 126/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.4174 - acc: 0.4035 - val_loss: 1.4258 - val_acc: 0.4076\n",
      "Epoch 127/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.3774 - acc: 0.3985 - val_loss: 1.4113 - val_acc: 0.4051\n",
      "Epoch 128/150\n",
      "6278/6278 [==============================] - 2s 376us/sample - loss: 1.4076 - acc: 0.4068 - val_loss: 1.4559 - val_acc: 0.4287\n",
      "Epoch 129/150\n",
      "6278/6278 [==============================] - 3s 400us/sample - loss: 1.3814 - acc: 0.4055 - val_loss: 1.3883 - val_acc: 0.4459\n",
      "Epoch 130/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.3764 - acc: 0.4100 - val_loss: 1.4083 - val_acc: 0.4287\n",
      "Epoch 131/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.3755 - acc: 0.4036 - val_loss: 1.4296 - val_acc: 0.4261\n",
      "Epoch 132/150\n",
      "6278/6278 [==============================] - 2s 379us/sample - loss: 1.3704 - acc: 0.4127 - val_loss: 1.4394 - val_acc: 0.4140\n",
      "Epoch 133/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.4281 - acc: 0.3944 - val_loss: 1.4040 - val_acc: 0.4134\n",
      "Epoch 134/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.4112 - acc: 0.4051 - val_loss: 1.4006 - val_acc: 0.4268\n",
      "Epoch 135/150\n",
      "6278/6278 [==============================] - 2s 377us/sample - loss: 1.3824 - acc: 0.3982 - val_loss: 1.3617 - val_acc: 0.4223\n",
      "Epoch 136/150\n",
      "6278/6278 [==============================] - 2s 389us/sample - loss: 1.3683 - acc: 0.4027 - val_loss: 1.3899 - val_acc: 0.4134\n",
      "Epoch 137/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.4156 - acc: 0.3952 - val_loss: 1.3753 - val_acc: 0.4350\n",
      "Epoch 138/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.4132 - acc: 0.3949 - val_loss: 1.4939 - val_acc: 0.3847\n",
      "Epoch 139/150\n",
      "6278/6278 [==============================] - 2s 390us/sample - loss: 1.4096 - acc: 0.3995 - val_loss: 1.4569 - val_acc: 0.4032\n",
      "Epoch 140/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.3835 - acc: 0.4095 - val_loss: 1.3634 - val_acc: 0.4268\n",
      "Epoch 141/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.3977 - acc: 0.4028 - val_loss: 1.3625 - val_acc: 0.4885\n",
      "Epoch 142/150\n",
      "6278/6278 [==============================] - 2s 392us/sample - loss: 1.3785 - acc: 0.4156 - val_loss: 1.4045 - val_acc: 0.4274\n",
      "Epoch 143/150\n",
      "6278/6278 [==============================] - 2s 384us/sample - loss: 1.3939 - acc: 0.4047 - val_loss: 1.4056 - val_acc: 0.4204\n",
      "Epoch 144/150\n",
      "6278/6278 [==============================] - 2s 394us/sample - loss: 1.3793 - acc: 0.4027 - val_loss: 1.3965 - val_acc: 0.4197\n",
      "Epoch 145/150\n",
      "6278/6278 [==============================] - 2s 385us/sample - loss: 1.3948 - acc: 0.4081 - val_loss: 1.4389 - val_acc: 0.4134\n",
      "Epoch 146/150\n",
      "6278/6278 [==============================] - 2s 381us/sample - loss: 1.3655 - acc: 0.4068 - val_loss: 1.4097 - val_acc: 0.4197\n",
      "Epoch 147/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.3423 - acc: 0.4161 - val_loss: 1.4302 - val_acc: 0.4357\n",
      "Epoch 148/150\n",
      "6278/6278 [==============================] - 2s 386us/sample - loss: 1.3766 - acc: 0.4151 - val_loss: 1.4163 - val_acc: 0.4701\n",
      "Epoch 149/150\n",
      "6278/6278 [==============================] - 2s 387us/sample - loss: 1.3717 - acc: 0.4148 - val_loss: 1.4015 - val_acc: 0.4076\n",
      "Epoch 150/150\n",
      "6278/6278 [==============================] - 2s 383us/sample - loss: 1.3518 - acc: 0.4043 - val_loss: 1.4200 - val_acc: 0.3860\n",
      "6278\n",
      "1570\n",
      "Train on 6279 samples, validate on 1569 samples\n",
      "Epoch 1/150\n",
      "6279/6279 [==============================] - 3s 509us/sample - loss: 2.8862 - acc: 0.1287 - val_loss: 2.5456 - val_acc: 0.0943\n",
      "Epoch 2/150\n",
      "6279/6279 [==============================] - 2s 381us/sample - loss: 2.5200 - acc: 0.1663 - val_loss: 2.1020 - val_acc: 0.2543\n",
      "Epoch 3/150\n",
      "6279/6279 [==============================] - 2s 376us/sample - loss: 2.2488 - acc: 0.2085 - val_loss: 2.1026 - val_acc: 0.2607\n",
      "Epoch 4/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 2.1704 - acc: 0.2381 - val_loss: 1.9375 - val_acc: 0.3454\n",
      "Epoch 5/150\n",
      "6279/6279 [==============================] - 2s 375us/sample - loss: 2.0692 - acc: 0.2572 - val_loss: 2.1776 - val_acc: 0.2734\n",
      "Epoch 6/150\n",
      "6279/6279 [==============================] - 2s 379us/sample - loss: 2.0750 - acc: 0.2642 - val_loss: 1.8571 - val_acc: 0.3250\n",
      "Epoch 7/150\n",
      "6279/6279 [==============================] - 2s 374us/sample - loss: 1.9727 - acc: 0.2879 - val_loss: 1.8268 - val_acc: 0.3308\n",
      "Epoch 8/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.9280 - acc: 0.2848 - val_loss: 1.8410 - val_acc: 0.3805\n",
      "Epoch 9/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.9140 - acc: 0.3018 - val_loss: 1.7997 - val_acc: 0.3505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150\n",
      "6279/6279 [==============================] - 2s 379us/sample - loss: 1.8304 - acc: 0.3212 - val_loss: 1.8140 - val_acc: 0.4073\n",
      "Epoch 11/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.8134 - acc: 0.3149 - val_loss: 1.8633 - val_acc: 0.3180\n",
      "Epoch 12/150\n",
      "6279/6279 [==============================] - 2s 373us/sample - loss: 1.7692 - acc: 0.3281 - val_loss: 1.6617 - val_acc: 0.3225\n",
      "Epoch 13/150\n",
      "6279/6279 [==============================] - 2s 377us/sample - loss: 1.7795 - acc: 0.3386 - val_loss: 1.7598 - val_acc: 0.3225\n",
      "Epoch 14/150\n",
      "6279/6279 [==============================] - 2s 378us/sample - loss: 1.7350 - acc: 0.3303 - val_loss: 1.7020 - val_acc: 0.3308\n",
      "Epoch 15/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.7888 - acc: 0.3284 - val_loss: 1.7150 - val_acc: 0.3703\n",
      "Epoch 16/150\n",
      "6279/6279 [==============================] - 2s 375us/sample - loss: 1.7304 - acc: 0.3330 - val_loss: 1.6365 - val_acc: 0.3901\n",
      "Epoch 17/150\n",
      "6279/6279 [==============================] - 2s 375us/sample - loss: 1.7410 - acc: 0.3263 - val_loss: 1.6746 - val_acc: 0.4398\n",
      "Epoch 18/150\n",
      "6279/6279 [==============================] - 2s 377us/sample - loss: 1.7104 - acc: 0.3526 - val_loss: 1.6266 - val_acc: 0.3684\n",
      "Epoch 19/150\n",
      "6279/6279 [==============================] - 2s 369us/sample - loss: 1.6828 - acc: 0.3426 - val_loss: 1.6094 - val_acc: 0.3907\n",
      "Epoch 20/150\n",
      "6279/6279 [==============================] - 2s 375us/sample - loss: 1.7325 - acc: 0.3423 - val_loss: 1.8819 - val_acc: 0.3665\n",
      "Epoch 21/150\n",
      "6279/6279 [==============================] - 2s 376us/sample - loss: 1.6512 - acc: 0.3572 - val_loss: 1.6431 - val_acc: 0.3384\n",
      "Epoch 22/150\n",
      "6279/6279 [==============================] - 2s 379us/sample - loss: 1.6387 - acc: 0.3526 - val_loss: 1.4961 - val_acc: 0.4047\n",
      "Epoch 23/150\n",
      "6279/6279 [==============================] - 2s 373us/sample - loss: 1.6815 - acc: 0.3512 - val_loss: 1.7509 - val_acc: 0.3601\n",
      "Epoch 24/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.6540 - acc: 0.3504 - val_loss: 1.6159 - val_acc: 0.4015\n",
      "Epoch 25/150\n",
      "6279/6279 [==============================] - 2s 384us/sample - loss: 1.6015 - acc: 0.3716 - val_loss: 1.5624 - val_acc: 0.3779\n",
      "Epoch 26/150\n",
      "6279/6279 [==============================] - 3s 402us/sample - loss: 1.6364 - acc: 0.3590 - val_loss: 1.5936 - val_acc: 0.3971\n",
      "Epoch 27/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.6251 - acc: 0.3606 - val_loss: 1.5684 - val_acc: 0.4347\n",
      "Epoch 28/150\n",
      "6279/6279 [==============================] - 3s 404us/sample - loss: 1.6214 - acc: 0.3567 - val_loss: 1.5816 - val_acc: 0.4640\n",
      "Epoch 29/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.5896 - acc: 0.3602 - val_loss: 1.6123 - val_acc: 0.4213\n",
      "Epoch 30/150\n",
      "6279/6279 [==============================] - 3s 413us/sample - loss: 1.6040 - acc: 0.3703 - val_loss: 1.6797 - val_acc: 0.3658\n",
      "Epoch 31/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.5854 - acc: 0.3639 - val_loss: 1.5450 - val_acc: 0.3977\n",
      "Epoch 32/150\n",
      "6279/6279 [==============================] - 3s 415us/sample - loss: 1.6233 - acc: 0.3631 - val_loss: 1.5403 - val_acc: 0.3627\n",
      "Epoch 33/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.6038 - acc: 0.3644 - val_loss: 1.5387 - val_acc: 0.3894\n",
      "Epoch 34/150\n",
      "6279/6279 [==============================] - 3s 408us/sample - loss: 1.5837 - acc: 0.3614 - val_loss: 1.5332 - val_acc: 0.4098\n",
      "Epoch 35/150\n",
      "6279/6279 [==============================] - 3s 403us/sample - loss: 1.5654 - acc: 0.3712 - val_loss: 1.5543 - val_acc: 0.3939\n",
      "Epoch 36/150\n",
      "6279/6279 [==============================] - 3s 399us/sample - loss: 1.5723 - acc: 0.3582 - val_loss: 1.5393 - val_acc: 0.4156\n",
      "Epoch 37/150\n",
      "6279/6279 [==============================] - 3s 412us/sample - loss: 1.5687 - acc: 0.3639 - val_loss: 1.5687 - val_acc: 0.3830\n",
      "Epoch 38/150\n",
      "6279/6279 [==============================] - 3s 398us/sample - loss: 1.5524 - acc: 0.3669 - val_loss: 1.5274 - val_acc: 0.3939\n",
      "Epoch 39/150\n",
      "6279/6279 [==============================] - 3s 406us/sample - loss: 1.5065 - acc: 0.3830 - val_loss: 1.4607 - val_acc: 0.4436\n",
      "Epoch 40/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.5468 - acc: 0.3736 - val_loss: 1.4907 - val_acc: 0.4117\n",
      "Epoch 41/150\n",
      "6279/6279 [==============================] - 3s 405us/sample - loss: 1.5782 - acc: 0.3668 - val_loss: 1.5005 - val_acc: 0.3932\n",
      "Epoch 42/150\n",
      "6279/6279 [==============================] - 2s 397us/sample - loss: 1.5125 - acc: 0.3582 - val_loss: 1.5678 - val_acc: 0.4041\n",
      "Epoch 43/150\n",
      "6279/6279 [==============================] - 3s 408us/sample - loss: 1.5300 - acc: 0.3798 - val_loss: 1.4711 - val_acc: 0.4321\n",
      "Epoch 44/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.5503 - acc: 0.3825 - val_loss: 1.4993 - val_acc: 0.3932\n",
      "Epoch 45/150\n",
      "6279/6279 [==============================] - 3s 414us/sample - loss: 1.5538 - acc: 0.3673 - val_loss: 1.5011 - val_acc: 0.4181\n",
      "Epoch 46/150\n",
      "6279/6279 [==============================] - 3s 399us/sample - loss: 1.5380 - acc: 0.3738 - val_loss: 1.5218 - val_acc: 0.4092\n",
      "Epoch 47/150\n",
      "6279/6279 [==============================] - 3s 408us/sample - loss: 1.5257 - acc: 0.3720 - val_loss: 1.5372 - val_acc: 0.3881\n",
      "Epoch 48/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.4904 - acc: 0.3787 - val_loss: 1.4513 - val_acc: 0.4245\n",
      "Epoch 49/150\n",
      "6279/6279 [==============================] - 3s 404us/sample - loss: 1.5405 - acc: 0.3813 - val_loss: 1.6206 - val_acc: 0.3372\n",
      "Epoch 50/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.4992 - acc: 0.3843 - val_loss: 1.4547 - val_acc: 0.4054\n",
      "Epoch 51/150\n",
      "6279/6279 [==============================] - 3s 411us/sample - loss: 1.5150 - acc: 0.3763 - val_loss: 1.4847 - val_acc: 0.4614\n",
      "Epoch 52/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.4870 - acc: 0.3881 - val_loss: 1.4398 - val_acc: 0.4143\n",
      "Epoch 53/150\n",
      "6279/6279 [==============================] - 3s 402us/sample - loss: 1.4698 - acc: 0.3805 - val_loss: 1.5538 - val_acc: 0.4544\n",
      "Epoch 54/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.5156 - acc: 0.3795 - val_loss: 1.4841 - val_acc: 0.3856\n",
      "Epoch 55/150\n",
      "6279/6279 [==============================] - 2s 397us/sample - loss: 1.4811 - acc: 0.3958 - val_loss: 1.5184 - val_acc: 0.4207\n",
      "Epoch 56/150\n",
      "6279/6279 [==============================] - 3s 410us/sample - loss: 1.5283 - acc: 0.3867 - val_loss: 1.4510 - val_acc: 0.4187\n",
      "Epoch 57/150\n",
      "6279/6279 [==============================] - ETA: 0s - loss: 1.4559 - acc: 0.385 - 2s 398us/sample - loss: 1.4558 - acc: 0.3865 - val_loss: 1.4450 - val_acc: 0.4149\n",
      "Epoch 58/150\n",
      "6279/6279 [==============================] - 3s 411us/sample - loss: 1.5055 - acc: 0.3808 - val_loss: 1.5909 - val_acc: 0.3837\n",
      "Epoch 59/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.4676 - acc: 0.3961 - val_loss: 1.5048 - val_acc: 0.4264\n",
      "Epoch 60/150\n",
      "6279/6279 [==============================] - 3s 407us/sample - loss: 1.4940 - acc: 0.3845 - val_loss: 1.4464 - val_acc: 0.4296\n",
      "Epoch 61/150\n",
      "6279/6279 [==============================] - 3s 399us/sample - loss: 1.4488 - acc: 0.3916 - val_loss: 1.4996 - val_acc: 0.3926\n",
      "Epoch 62/150\n",
      "6279/6279 [==============================] - 3s 414us/sample - loss: 1.4961 - acc: 0.3875 - val_loss: 1.5485 - val_acc: 0.4226\n",
      "Epoch 63/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.4487 - acc: 0.3916 - val_loss: 1.5962 - val_acc: 0.4321\n",
      "Epoch 64/150\n",
      "6279/6279 [==============================] - 3s 411us/sample - loss: 1.4649 - acc: 0.3827 - val_loss: 1.4679 - val_acc: 0.4219\n",
      "Epoch 65/150\n",
      "6279/6279 [==============================] - 3s 403us/sample - loss: 1.4514 - acc: 0.3896 - val_loss: 1.4166 - val_acc: 0.4589\n",
      "Epoch 66/150\n",
      "6279/6279 [==============================] - 3s 414us/sample - loss: 1.4757 - acc: 0.3876 - val_loss: 1.4638 - val_acc: 0.4270\n",
      "Epoch 67/150\n",
      "6279/6279 [==============================] - 3s 402us/sample - loss: 1.4534 - acc: 0.3851 - val_loss: 1.5003 - val_acc: 0.4321\n",
      "Epoch 68/150\n",
      "6279/6279 [==============================] - 3s 409us/sample - loss: 1.4646 - acc: 0.3927 - val_loss: 1.4468 - val_acc: 0.4385\n",
      "Epoch 69/150\n",
      "6279/6279 [==============================] - 2s 397us/sample - loss: 1.4864 - acc: 0.4028 - val_loss: 1.4827 - val_acc: 0.4168\n",
      "Epoch 70/150\n",
      "6279/6279 [==============================] - 3s 403us/sample - loss: 1.4661 - acc: 0.3870 - val_loss: 1.5436 - val_acc: 0.4028\n",
      "Epoch 71/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.4642 - acc: 0.3848 - val_loss: 1.4335 - val_acc: 0.4136\n",
      "Epoch 72/150\n",
      "6279/6279 [==============================] - 3s 412us/sample - loss: 1.4335 - acc: 0.3919 - val_loss: 1.4664 - val_acc: 0.4168\n",
      "Epoch 73/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.4644 - acc: 0.3899 - val_loss: 1.4441 - val_acc: 0.4334\n",
      "Epoch 74/150\n",
      "6279/6279 [==============================] - 3s 398us/sample - loss: 1.4493 - acc: 0.3773 - val_loss: 1.4757 - val_acc: 0.4073\n",
      "Epoch 75/150\n",
      "6279/6279 [==============================] - 3s 408us/sample - loss: 1.4713 - acc: 0.3889 - val_loss: 1.4588 - val_acc: 0.4257\n",
      "Epoch 76/150\n",
      "6279/6279 [==============================] - 3s 399us/sample - loss: 1.4728 - acc: 0.3929 - val_loss: 1.4508 - val_acc: 0.4015\n",
      "Epoch 77/150\n",
      "6279/6279 [==============================] - 3s 405us/sample - loss: 1.4612 - acc: 0.3939 - val_loss: 1.4822 - val_acc: 0.3843\n",
      "Epoch 78/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.4066 - acc: 0.3948 - val_loss: 1.4339 - val_acc: 0.4500\n",
      "Epoch 79/150\n",
      "6279/6279 [==============================] - 3s 408us/sample - loss: 1.4455 - acc: 0.4147 - val_loss: 1.4806 - val_acc: 0.4481\n",
      "Epoch 80/150\n",
      "6279/6279 [==============================] - 3s 405us/sample - loss: 1.4459 - acc: 0.4107 - val_loss: 1.4378 - val_acc: 0.3907\n",
      "Epoch 81/150\n",
      "6279/6279 [==============================] - 3s 407us/sample - loss: 1.4554 - acc: 0.4068 - val_loss: 1.4266 - val_acc: 0.3945\n",
      "Epoch 82/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.4503 - acc: 0.3843 - val_loss: 1.4386 - val_acc: 0.4417\n",
      "Epoch 83/150\n",
      "6279/6279 [==============================] - 3s 408us/sample - loss: 1.4572 - acc: 0.4031 - val_loss: 1.4434 - val_acc: 0.4347\n",
      "Epoch 84/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.4512 - acc: 0.3985 - val_loss: 1.5316 - val_acc: 0.3805\n",
      "Epoch 85/150\n",
      "6279/6279 [==============================] - 3s 408us/sample - loss: 1.4206 - acc: 0.3926 - val_loss: 1.4452 - val_acc: 0.3837\n",
      "Epoch 86/150\n",
      "6279/6279 [==============================] - 3s 401us/sample - loss: 1.4018 - acc: 0.3923 - val_loss: 1.4232 - val_acc: 0.4391\n",
      "Epoch 87/150\n",
      "6279/6279 [==============================] - 3s 410us/sample - loss: 1.4182 - acc: 0.4007 - val_loss: 1.3848 - val_acc: 0.4238\n",
      "Epoch 88/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.3948 - acc: 0.4096 - val_loss: 1.3792 - val_acc: 0.4283\n",
      "Epoch 89/150\n",
      "6279/6279 [==============================] - 3s 408us/sample - loss: 1.4254 - acc: 0.3999 - val_loss: 1.4715 - val_acc: 0.4308\n",
      "Epoch 90/150\n",
      "6279/6279 [==============================] - 3s 402us/sample - loss: 1.4197 - acc: 0.4029 - val_loss: 1.4048 - val_acc: 0.4366\n",
      "Epoch 91/150\n",
      "6279/6279 [==============================] - 3s 409us/sample - loss: 1.4313 - acc: 0.3864 - val_loss: 1.4515 - val_acc: 0.4487\n",
      "Epoch 92/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.4571 - acc: 0.3940 - val_loss: 1.5075 - val_acc: 0.4461\n",
      "Epoch 93/150\n",
      "6279/6279 [==============================] - 3s 413us/sample - loss: 1.4195 - acc: 0.4061 - val_loss: 1.5360 - val_acc: 0.3869\n",
      "Epoch 94/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.4380 - acc: 0.3862 - val_loss: 1.4244 - val_acc: 0.4111\n",
      "Epoch 95/150\n",
      "6279/6279 [==============================] - 3s 411us/sample - loss: 1.4180 - acc: 0.3958 - val_loss: 1.4580 - val_acc: 0.4417\n",
      "Epoch 96/150\n",
      "6279/6279 [==============================] - 3s 403us/sample - loss: 1.4305 - acc: 0.4138 - val_loss: 1.4598 - val_acc: 0.4410\n",
      "Epoch 97/150\n",
      "6279/6279 [==============================] - 3s 402us/sample - loss: 1.3997 - acc: 0.4058 - val_loss: 1.3888 - val_acc: 0.4149\n",
      "Epoch 98/150\n",
      "6279/6279 [==============================] - 3s 404us/sample - loss: 1.4129 - acc: 0.3910 - val_loss: 1.4314 - val_acc: 0.4277\n",
      "Epoch 99/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.4113 - acc: 0.3886 - val_loss: 1.3829 - val_acc: 0.4512\n",
      "Epoch 100/150\n",
      "6279/6279 [==============================] - 3s 404us/sample - loss: 1.4153 - acc: 0.4056 - val_loss: 1.4669 - val_acc: 0.4430\n",
      "Epoch 101/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.3781 - acc: 0.3985 - val_loss: 1.4088 - val_acc: 0.4481\n",
      "Epoch 102/150\n",
      "6279/6279 [==============================] - 2s 392us/sample - loss: 1.4171 - acc: 0.3935 - val_loss: 1.4300 - val_acc: 0.4105\n",
      "Epoch 103/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.3763 - acc: 0.4064 - val_loss: 1.3750 - val_acc: 0.4519\n",
      "Epoch 104/150\n",
      "6279/6279 [==============================] - 2s 381us/sample - loss: 1.4285 - acc: 0.3956 - val_loss: 1.4807 - val_acc: 0.3888\n",
      "Epoch 105/150\n",
      "6279/6279 [==============================] - 2s 375us/sample - loss: 1.4243 - acc: 0.3884 - val_loss: 1.4498 - val_acc: 0.4455\n",
      "Epoch 106/150\n",
      "6279/6279 [==============================] - 2s 378us/sample - loss: 1.3897 - acc: 0.4032 - val_loss: 1.3898 - val_acc: 0.4436\n",
      "Epoch 107/150\n",
      "6279/6279 [==============================] - 2s 378us/sample - loss: 1.3937 - acc: 0.4075 - val_loss: 1.4102 - val_acc: 0.4404\n",
      "Epoch 108/150\n",
      "6279/6279 [==============================] - 2s 379us/sample - loss: 1.4198 - acc: 0.4025 - val_loss: 1.4548 - val_acc: 0.4602\n",
      "Epoch 109/150\n",
      "6279/6279 [==============================] - 2s 371us/sample - loss: 1.4459 - acc: 0.4064 - val_loss: 1.4199 - val_acc: 0.4302\n",
      "Epoch 110/150\n",
      "6279/6279 [==============================] - 2s 374us/sample - loss: 1.4094 - acc: 0.3969 - val_loss: 1.4431 - val_acc: 0.4283\n",
      "Epoch 111/150\n",
      "6279/6279 [==============================] - 2s 380us/sample - loss: 1.4075 - acc: 0.4142 - val_loss: 1.3857 - val_acc: 0.4442\n",
      "Epoch 112/150\n",
      "6279/6279 [==============================] - 2s 376us/sample - loss: 1.3901 - acc: 0.4149 - val_loss: 1.4016 - val_acc: 0.4627\n",
      "Epoch 113/150\n",
      "6279/6279 [==============================] - 2s 379us/sample - loss: 1.3954 - acc: 0.4066 - val_loss: 1.4398 - val_acc: 0.4302\n",
      "Epoch 114/150\n",
      "6279/6279 [==============================] - 2s 376us/sample - loss: 1.3728 - acc: 0.4206 - val_loss: 1.4326 - val_acc: 0.4092\n",
      "Epoch 115/150\n",
      "6279/6279 [==============================] - 2s 380us/sample - loss: 1.4111 - acc: 0.3978 - val_loss: 1.4194 - val_acc: 0.4321\n",
      "Epoch 116/150\n",
      "6279/6279 [==============================] - 2s 375us/sample - loss: 1.4012 - acc: 0.4032 - val_loss: 1.4203 - val_acc: 0.4296\n",
      "Epoch 117/150\n",
      "6279/6279 [==============================] - 2s 378us/sample - loss: 1.3766 - acc: 0.4112 - val_loss: 1.4196 - val_acc: 0.4226\n",
      "Epoch 118/150\n",
      "6279/6279 [==============================] - 2s 374us/sample - loss: 1.3449 - acc: 0.4173 - val_loss: 1.4354 - val_acc: 0.4519\n",
      "Epoch 119/150\n",
      "6279/6279 [==============================] - 2s 384us/sample - loss: 1.3811 - acc: 0.3999 - val_loss: 1.4306 - val_acc: 0.4442\n",
      "Epoch 120/150\n",
      "6279/6279 [==============================] - 2s 376us/sample - loss: 1.3879 - acc: 0.4087 - val_loss: 1.3737 - val_acc: 0.4423\n",
      "Epoch 121/150\n",
      "6279/6279 [==============================] - 2s 372us/sample - loss: 1.3721 - acc: 0.4150 - val_loss: 1.4094 - val_acc: 0.4391\n",
      "Epoch 122/150\n",
      "6279/6279 [==============================] - 2s 379us/sample - loss: 1.4304 - acc: 0.4093 - val_loss: 1.4656 - val_acc: 0.4003\n",
      "Epoch 123/150\n",
      "6279/6279 [==============================] - 2s 371us/sample - loss: 1.3670 - acc: 0.4083 - val_loss: 1.4058 - val_acc: 0.4417\n",
      "Epoch 124/150\n",
      "6279/6279 [==============================] - 2s 380us/sample - loss: 1.3693 - acc: 0.4061 - val_loss: 1.4406 - val_acc: 0.4487\n",
      "Epoch 125/150\n",
      "6279/6279 [==============================] - 2s 376us/sample - loss: 1.3730 - acc: 0.4031 - val_loss: 1.4470 - val_acc: 0.4385\n",
      "Epoch 126/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6279/6279 [==============================] - 2s 379us/sample - loss: 1.3663 - acc: 0.4166 - val_loss: 1.3844 - val_acc: 0.4417\n",
      "Epoch 127/150\n",
      "6279/6279 [==============================] - 2s 374us/sample - loss: 1.3453 - acc: 0.4107 - val_loss: 1.4158 - val_acc: 0.4551\n",
      "Epoch 128/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.3769 - acc: 0.4055 - val_loss: 1.4463 - val_acc: 0.4506\n",
      "Epoch 129/150\n",
      "6279/6279 [==============================] - 2s 370us/sample - loss: 1.3588 - acc: 0.4193 - val_loss: 1.5101 - val_acc: 0.4079\n",
      "Epoch 130/150\n",
      "6279/6279 [==============================] - 2s 372us/sample - loss: 1.3633 - acc: 0.4077 - val_loss: 1.4278 - val_acc: 0.4487\n",
      "Epoch 131/150\n",
      "6279/6279 [==============================] - 2s 376us/sample - loss: 1.3981 - acc: 0.4029 - val_loss: 1.4042 - val_acc: 0.4315\n",
      "Epoch 132/150\n",
      "6279/6279 [==============================] - 2s 372us/sample - loss: 1.3405 - acc: 0.4099 - val_loss: 1.4059 - val_acc: 0.4474\n",
      "Epoch 133/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.3778 - acc: 0.4197 - val_loss: 1.4488 - val_acc: 0.4187\n",
      "Epoch 134/150\n",
      "6279/6279 [==============================] - 2s 372us/sample - loss: 1.3931 - acc: 0.4064 - val_loss: 1.4071 - val_acc: 0.4423\n",
      "Epoch 135/150\n",
      "6279/6279 [==============================] - 2s 380us/sample - loss: 1.3610 - acc: 0.4128 - val_loss: 1.4117 - val_acc: 0.4481\n",
      "Epoch 136/150\n",
      "6279/6279 [==============================] - 2s 375us/sample - loss: 1.3860 - acc: 0.4066 - val_loss: 1.4223 - val_acc: 0.4181\n",
      "Epoch 137/150\n",
      "6279/6279 [==============================] - 2s 377us/sample - loss: 1.3867 - acc: 0.4075 - val_loss: 1.3718 - val_acc: 0.4455\n",
      "Epoch 138/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.3408 - acc: 0.4212 - val_loss: 1.4278 - val_acc: 0.4296\n",
      "Epoch 139/150\n",
      "6279/6279 [==============================] - 2s 384us/sample - loss: 1.3471 - acc: 0.4134 - val_loss: 1.4967 - val_acc: 0.4321\n",
      "Epoch 140/150\n",
      "6279/6279 [==============================] - 2s 375us/sample - loss: 1.3922 - acc: 0.4101 - val_loss: 1.4260 - val_acc: 0.4200\n",
      "Epoch 141/150\n",
      "6279/6279 [==============================] - 2s 379us/sample - loss: 1.3687 - acc: 0.4131 - val_loss: 1.4278 - val_acc: 0.4232\n",
      "Epoch 142/150\n",
      "6279/6279 [==============================] - 2s 380us/sample - loss: 1.3494 - acc: 0.4216 - val_loss: 1.4254 - val_acc: 0.4136\n",
      "Epoch 143/150\n",
      "6279/6279 [==============================] - 2s 372us/sample - loss: 1.3820 - acc: 0.4115 - val_loss: 1.4042 - val_acc: 0.4162\n",
      "Epoch 144/150\n",
      "6279/6279 [==============================] - 2s 376us/sample - loss: 1.3764 - acc: 0.4125 - val_loss: 1.5160 - val_acc: 0.4117\n",
      "Epoch 145/150\n",
      "6279/6279 [==============================] - 2s 373us/sample - loss: 1.3860 - acc: 0.4034 - val_loss: 1.3948 - val_acc: 0.4551\n",
      "Epoch 146/150\n",
      "6279/6279 [==============================] - 2s 378us/sample - loss: 1.3399 - acc: 0.4222 - val_loss: 1.4470 - val_acc: 0.4302\n",
      "Epoch 147/150\n",
      "6279/6279 [==============================] - 2s 372us/sample - loss: 1.3579 - acc: 0.4093 - val_loss: 1.4088 - val_acc: 0.4410\n",
      "Epoch 148/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.4049 - acc: 0.4149 - val_loss: 1.4694 - val_acc: 0.4079\n",
      "Epoch 149/150\n",
      "6279/6279 [==============================] - 2s 372us/sample - loss: 1.3808 - acc: 0.4174 - val_loss: 1.4274 - val_acc: 0.4512\n",
      "Epoch 150/150\n",
      "6279/6279 [==============================] - 2s 372us/sample - loss: 1.3993 - acc: 0.4099 - val_loss: 1.4295 - val_acc: 0.4143\n",
      "6279\n",
      "1569\n",
      "Train on 6279 samples, validate on 1569 samples\n",
      "Epoch 1/150\n",
      "6279/6279 [==============================] - 3s 538us/sample - loss: 2.8107 - acc: 0.1444 - val_loss: 2.8798 - val_acc: 0.2562\n",
      "Epoch 2/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 2.4056 - acc: 0.1879 - val_loss: 2.4940 - val_acc: 0.2473\n",
      "Epoch 3/150\n",
      "6279/6279 [==============================] - 2s 385us/sample - loss: 2.2482 - acc: 0.2177 - val_loss: 2.3990 - val_acc: 0.2989\n",
      "Epoch 4/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 2.0922 - acc: 0.2500 - val_loss: 2.2092 - val_acc: 0.2804\n",
      "Epoch 5/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 2.0025 - acc: 0.2655 - val_loss: 2.1141 - val_acc: 0.3537\n",
      "Epoch 6/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.9383 - acc: 0.2942 - val_loss: 2.0814 - val_acc: 0.4034\n",
      "Epoch 7/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.8778 - acc: 0.3031 - val_loss: 2.1121 - val_acc: 0.3601\n",
      "Epoch 8/150\n",
      "6279/6279 [==============================] - 2s 397us/sample - loss: 1.8254 - acc: 0.3136 - val_loss: 2.0293 - val_acc: 0.3544\n",
      "Epoch 9/150\n",
      "6279/6279 [==============================] - 2s 387us/sample - loss: 1.8320 - acc: 0.3102 - val_loss: 1.9620 - val_acc: 0.3945\n",
      "Epoch 10/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.7690 - acc: 0.3251 - val_loss: 2.0015 - val_acc: 0.4742\n",
      "Epoch 11/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.7550 - acc: 0.3254 - val_loss: 2.0203 - val_acc: 0.3493\n",
      "Epoch 12/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.7175 - acc: 0.3314 - val_loss: 1.9280 - val_acc: 0.3741\n",
      "Epoch 13/150\n",
      "6279/6279 [==============================] - 2s 384us/sample - loss: 1.7138 - acc: 0.3408 - val_loss: 2.3228 - val_acc: 0.3040\n",
      "Epoch 14/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.7461 - acc: 0.3292 - val_loss: 2.1725 - val_acc: 0.3308\n",
      "Epoch 15/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.6726 - acc: 0.3475 - val_loss: 2.0640 - val_acc: 0.3856\n",
      "Epoch 16/150\n",
      "6279/6279 [==============================] - 3s 411us/sample - loss: 1.6723 - acc: 0.3464 - val_loss: 1.8006 - val_acc: 0.4296\n",
      "Epoch 17/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.6195 - acc: 0.3418 - val_loss: 1.9579 - val_acc: 0.3665\n",
      "Epoch 18/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.6503 - acc: 0.3515 - val_loss: 1.9794 - val_acc: 0.4079\n",
      "Epoch 19/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.6609 - acc: 0.3561 - val_loss: 1.9930 - val_acc: 0.3199\n",
      "Epoch 20/150\n",
      "6279/6279 [==============================] - 2s 390us/sample - loss: 1.6199 - acc: 0.3628 - val_loss: 2.0429 - val_acc: 0.4340\n",
      "Epoch 21/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.6137 - acc: 0.3612 - val_loss: 1.8037 - val_acc: 0.4162\n",
      "Epoch 22/150\n",
      "6279/6279 [==============================] - 2s 392us/sample - loss: 1.5934 - acc: 0.3703 - val_loss: 1.8711 - val_acc: 0.4245\n",
      "Epoch 23/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.5703 - acc: 0.3665 - val_loss: 1.8372 - val_acc: 0.3990\n",
      "Epoch 24/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.5725 - acc: 0.3693 - val_loss: 2.0426 - val_acc: 0.4410\n",
      "Epoch 25/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.5468 - acc: 0.3767 - val_loss: 1.7836 - val_acc: 0.4098\n",
      "Epoch 26/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.5907 - acc: 0.3599 - val_loss: 1.8479 - val_acc: 0.3901\n",
      "Epoch 27/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.5578 - acc: 0.3776 - val_loss: 1.8670 - val_acc: 0.3875\n",
      "Epoch 28/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.5674 - acc: 0.3630 - val_loss: 1.9420 - val_acc: 0.3830\n",
      "Epoch 29/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.5652 - acc: 0.3631 - val_loss: 1.8676 - val_acc: 0.3926\n",
      "Epoch 30/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.5306 - acc: 0.3712 - val_loss: 1.7346 - val_acc: 0.4506\n",
      "Epoch 31/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.5295 - acc: 0.3625 - val_loss: 1.7773 - val_acc: 0.4219\n",
      "Epoch 32/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.5514 - acc: 0.3647 - val_loss: 1.8146 - val_acc: 0.4187\n",
      "Epoch 33/150\n",
      "6279/6279 [==============================] - 3s 405us/sample - loss: 1.5050 - acc: 0.3862 - val_loss: 1.7878 - val_acc: 0.4251\n",
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.5219 - acc: 0.3803 - val_loss: 1.7816 - val_acc: 0.4251\n",
      "Epoch 35/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.5155 - acc: 0.3833 - val_loss: 1.7975 - val_acc: 0.4340\n",
      "Epoch 36/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.4827 - acc: 0.3926 - val_loss: 1.6914 - val_acc: 0.4876\n",
      "Epoch 37/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.5020 - acc: 0.3749 - val_loss: 1.7698 - val_acc: 0.4716\n",
      "Epoch 38/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.5253 - acc: 0.3767 - val_loss: 1.7377 - val_acc: 0.4474\n",
      "Epoch 39/150\n",
      "6279/6279 [==============================] - 2s 385us/sample - loss: 1.4746 - acc: 0.3824 - val_loss: 1.7861 - val_acc: 0.4066\n",
      "Epoch 40/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.5043 - acc: 0.3749 - val_loss: 1.7384 - val_acc: 0.3990\n",
      "Epoch 41/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.5043 - acc: 0.3736 - val_loss: 1.6927 - val_acc: 0.4232\n",
      "Epoch 42/150\n",
      "6279/6279 [==============================] - 2s 390us/sample - loss: 1.4931 - acc: 0.3810 - val_loss: 1.7399 - val_acc: 0.4009\n",
      "Epoch 43/150\n",
      "6279/6279 [==============================] - 2s 381us/sample - loss: 1.5084 - acc: 0.3872 - val_loss: 1.7126 - val_acc: 0.4296\n",
      "Epoch 44/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.4919 - acc: 0.3841 - val_loss: 1.7559 - val_acc: 0.4334\n",
      "Epoch 45/150\n",
      "6279/6279 [==============================] - 2s 387us/sample - loss: 1.4815 - acc: 0.3862 - val_loss: 1.7699 - val_acc: 0.4251\n",
      "Epoch 46/150\n",
      "6279/6279 [==============================] - 3s 402us/sample - loss: 1.4805 - acc: 0.3816 - val_loss: 1.9450 - val_acc: 0.4308\n",
      "Epoch 47/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.4788 - acc: 0.3803 - val_loss: 1.7051 - val_acc: 0.4302\n",
      "Epoch 48/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.4597 - acc: 0.3980 - val_loss: 1.7086 - val_acc: 0.4213\n",
      "Epoch 49/150\n",
      "6279/6279 [==============================] - 3s 401us/sample - loss: 1.4866 - acc: 0.3833 - val_loss: 1.6772 - val_acc: 0.4156\n",
      "Epoch 50/150\n",
      "6279/6279 [==============================] - 3s 401us/sample - loss: 1.4677 - acc: 0.3825 - val_loss: 1.7369 - val_acc: 0.4213\n",
      "Epoch 51/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.4529 - acc: 0.3814 - val_loss: 1.6330 - val_acc: 0.4347\n",
      "Epoch 52/150\n",
      "6279/6279 [==============================] - 3s 399us/sample - loss: 1.4566 - acc: 0.3873 - val_loss: 1.6821 - val_acc: 0.5029\n",
      "Epoch 53/150\n",
      "6279/6279 [==============================] - 2s 384us/sample - loss: 1.4395 - acc: 0.3994 - val_loss: 1.6889 - val_acc: 0.4270\n",
      "Epoch 54/150\n",
      "6279/6279 [==============================] - 2s 390us/sample - loss: 1.4341 - acc: 0.3997 - val_loss: 1.7139 - val_acc: 0.4436\n",
      "Epoch 55/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.4557 - acc: 0.3950 - val_loss: 1.7552 - val_acc: 0.4143\n",
      "Epoch 56/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.4298 - acc: 0.3943 - val_loss: 1.6261 - val_acc: 0.4474\n",
      "Epoch 57/150\n",
      "6279/6279 [==============================] - 2s 390us/sample - loss: 1.4335 - acc: 0.3787 - val_loss: 1.6721 - val_acc: 0.4308\n",
      "Epoch 58/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.4248 - acc: 0.3932 - val_loss: 1.6473 - val_acc: 0.4423\n",
      "Epoch 59/150\n",
      "6279/6279 [==============================] - 2s 397us/sample - loss: 1.4421 - acc: 0.4025 - val_loss: 1.6944 - val_acc: 0.4257\n",
      "Epoch 60/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.4110 - acc: 0.3948 - val_loss: 1.6452 - val_acc: 0.4251\n",
      "Epoch 61/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.4423 - acc: 0.3902 - val_loss: 1.7179 - val_acc: 0.4245\n",
      "Epoch 62/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.4605 - acc: 0.3905 - val_loss: 1.6042 - val_acc: 0.4436\n",
      "Epoch 63/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.4294 - acc: 0.3980 - val_loss: 1.6665 - val_acc: 0.3977\n",
      "Epoch 64/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.4151 - acc: 0.3825 - val_loss: 1.6050 - val_acc: 0.4761\n",
      "Epoch 65/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.3813 - acc: 0.4044 - val_loss: 1.7005 - val_acc: 0.4315\n",
      "Epoch 66/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.4146 - acc: 0.3908 - val_loss: 1.7234 - val_acc: 0.4181\n",
      "Epoch 67/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.3805 - acc: 0.4071 - val_loss: 1.6252 - val_acc: 0.4423\n",
      "Epoch 68/150\n",
      "6279/6279 [==============================] - 2s 387us/sample - loss: 1.4226 - acc: 0.3945 - val_loss: 1.7063 - val_acc: 0.4149\n",
      "Epoch 69/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.4232 - acc: 0.3900 - val_loss: 1.6787 - val_acc: 0.4423\n",
      "Epoch 70/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.4114 - acc: 0.3989 - val_loss: 1.7282 - val_acc: 0.4079\n",
      "Epoch 71/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.4178 - acc: 0.3967 - val_loss: 1.6724 - val_acc: 0.4334\n",
      "Epoch 72/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.4216 - acc: 0.3945 - val_loss: 1.7566 - val_acc: 0.4359\n",
      "Epoch 73/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.4228 - acc: 0.4068 - val_loss: 1.6536 - val_acc: 0.4614\n",
      "Epoch 74/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.4292 - acc: 0.3916 - val_loss: 1.6746 - val_acc: 0.4251\n",
      "Epoch 75/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.4117 - acc: 0.3875 - val_loss: 1.6227 - val_acc: 0.4251\n",
      "Epoch 76/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.3816 - acc: 0.3931 - val_loss: 1.6698 - val_acc: 0.4289\n",
      "Epoch 77/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.4095 - acc: 0.3856 - val_loss: 1.6066 - val_acc: 0.4589\n",
      "Epoch 78/150\n",
      "6279/6279 [==============================] - 2s 392us/sample - loss: 1.3900 - acc: 0.3916 - val_loss: 1.5869 - val_acc: 0.4366\n",
      "Epoch 79/150\n",
      "6279/6279 [==============================] - 2s 379us/sample - loss: 1.4195 - acc: 0.3980 - val_loss: 1.6671 - val_acc: 0.4366\n",
      "Epoch 80/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.3905 - acc: 0.3975 - val_loss: 1.6210 - val_acc: 0.4124\n",
      "Epoch 81/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.4015 - acc: 0.3983 - val_loss: 1.6304 - val_acc: 0.4506\n",
      "Epoch 82/150\n",
      "6279/6279 [==============================] - 2s 390us/sample - loss: 1.3934 - acc: 0.3953 - val_loss: 1.6668 - val_acc: 0.4512\n",
      "Epoch 83/150\n",
      "6279/6279 [==============================] - 2s 387us/sample - loss: 1.3817 - acc: 0.4160 - val_loss: 1.5774 - val_acc: 0.4474\n",
      "Epoch 84/150\n",
      "6279/6279 [==============================] - 2s 397us/sample - loss: 1.4109 - acc: 0.3993 - val_loss: 1.6325 - val_acc: 0.4366\n",
      "Epoch 85/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.3737 - acc: 0.4115 - val_loss: 1.6076 - val_acc: 0.4270\n",
      "Epoch 86/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.3831 - acc: 0.4010 - val_loss: 1.6043 - val_acc: 0.4289\n",
      "Epoch 87/150\n",
      "6279/6279 [==============================] - 3s 399us/sample - loss: 1.3836 - acc: 0.3962 - val_loss: 1.6461 - val_acc: 0.4481\n",
      "Epoch 88/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.4121 - acc: 0.3970 - val_loss: 1.6968 - val_acc: 0.4538\n",
      "Epoch 89/150\n",
      "6279/6279 [==============================] - 2s 390us/sample - loss: 1.3718 - acc: 0.4005 - val_loss: 1.6359 - val_acc: 0.4238\n",
      "Epoch 90/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.3963 - acc: 0.3943 - val_loss: 1.6153 - val_acc: 0.4251\n",
      "Epoch 91/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.3907 - acc: 0.4095 - val_loss: 1.6530 - val_acc: 0.4162\n",
      "Epoch 92/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.3960 - acc: 0.3996 - val_loss: 1.5931 - val_acc: 0.4544\n",
      "Epoch 93/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.3940 - acc: 0.4028 - val_loss: 1.6097 - val_acc: 0.4334\n",
      "Epoch 94/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.3739 - acc: 0.3972 - val_loss: 1.5872 - val_acc: 0.4799\n",
      "Epoch 95/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.4038 - acc: 0.3889 - val_loss: 1.6078 - val_acc: 0.4627\n",
      "Epoch 96/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.3687 - acc: 0.4047 - val_loss: 1.6125 - val_acc: 0.4500\n",
      "Epoch 97/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.3482 - acc: 0.4099 - val_loss: 1.6437 - val_acc: 0.4512\n",
      "Epoch 98/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.4158 - acc: 0.3982 - val_loss: 1.6205 - val_acc: 0.4691\n",
      "Epoch 99/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.3812 - acc: 0.3972 - val_loss: 1.6345 - val_acc: 0.4780\n",
      "Epoch 100/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.3736 - acc: 0.4071 - val_loss: 1.6308 - val_acc: 0.4729\n",
      "Epoch 101/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.3821 - acc: 0.4026 - val_loss: 1.5968 - val_acc: 0.4366\n",
      "Epoch 102/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.3462 - acc: 0.4040 - val_loss: 1.7356 - val_acc: 0.4066\n",
      "Epoch 103/150\n",
      "6279/6279 [==============================] - 2s 381us/sample - loss: 1.3853 - acc: 0.4109 - val_loss: 1.5782 - val_acc: 0.5035\n",
      "Epoch 104/150\n",
      "6279/6279 [==============================] - 2s 397us/sample - loss: 1.3710 - acc: 0.4141 - val_loss: 1.5769 - val_acc: 0.4353\n",
      "Epoch 105/150\n",
      "6279/6279 [==============================] - 2s 383us/sample - loss: 1.3682 - acc: 0.3964 - val_loss: 1.5438 - val_acc: 0.4634\n",
      "Epoch 106/150\n",
      "6279/6279 [==============================] - 2s 392us/sample - loss: 1.3807 - acc: 0.4104 - val_loss: 1.5692 - val_acc: 0.4398\n",
      "Epoch 107/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.3120 - acc: 0.4123 - val_loss: 1.5665 - val_acc: 0.4857\n",
      "Epoch 108/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.3678 - acc: 0.4025 - val_loss: 1.6223 - val_acc: 0.4296\n",
      "Epoch 109/150\n",
      "6279/6279 [==============================] - 2s 390us/sample - loss: 1.3570 - acc: 0.4052 - val_loss: 1.6262 - val_acc: 0.4162\n",
      "Epoch 110/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.3605 - acc: 0.4036 - val_loss: 1.5708 - val_acc: 0.4461\n",
      "Epoch 111/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.3614 - acc: 0.4066 - val_loss: 1.6992 - val_acc: 0.4455\n",
      "Epoch 112/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.3876 - acc: 0.4039 - val_loss: 1.6277 - val_acc: 0.4786\n",
      "Epoch 113/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.3854 - acc: 0.4131 - val_loss: 1.6255 - val_acc: 0.4487\n",
      "Epoch 114/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.3246 - acc: 0.4098 - val_loss: 1.6001 - val_acc: 0.4716\n",
      "Epoch 115/150\n",
      "6279/6279 [==============================] - 2s 381us/sample - loss: 1.3628 - acc: 0.4061 - val_loss: 1.6226 - val_acc: 0.4245\n",
      "Epoch 116/150\n",
      "6279/6279 [==============================] - 2s 380us/sample - loss: 1.3684 - acc: 0.4039 - val_loss: 1.6017 - val_acc: 0.4442\n",
      "Epoch 117/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.3700 - acc: 0.4015 - val_loss: 1.7181 - val_acc: 0.4423\n",
      "Epoch 118/150\n",
      "6279/6279 [==============================] - 2s 383us/sample - loss: 1.3524 - acc: 0.4075 - val_loss: 1.6077 - val_acc: 0.4812\n",
      "Epoch 119/150\n",
      "6279/6279 [==============================] - 2s 387us/sample - loss: 1.3544 - acc: 0.4031 - val_loss: 1.6425 - val_acc: 0.4404\n",
      "Epoch 120/150\n",
      "6279/6279 [==============================] - 2s 384us/sample - loss: 1.3454 - acc: 0.4021 - val_loss: 1.6356 - val_acc: 0.5010\n",
      "Epoch 121/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.4060 - acc: 0.4077 - val_loss: 1.5890 - val_acc: 0.4474\n",
      "Epoch 122/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.3442 - acc: 0.4133 - val_loss: 1.5975 - val_acc: 0.4627\n",
      "Epoch 123/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.3537 - acc: 0.4142 - val_loss: 1.6109 - val_acc: 0.4959\n",
      "Epoch 124/150\n",
      "6279/6279 [==============================] - 2s 384us/sample - loss: 1.3272 - acc: 0.4152 - val_loss: 1.6529 - val_acc: 0.4232\n",
      "Epoch 125/150\n",
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.3462 - acc: 0.4088 - val_loss: 1.7086 - val_acc: 0.4653\n",
      "Epoch 126/150\n",
      "6279/6279 [==============================] - 2s 382us/sample - loss: 1.3271 - acc: 0.4255 - val_loss: 1.5623 - val_acc: 0.4589\n",
      "Epoch 127/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.3683 - acc: 0.4071 - val_loss: 1.5716 - val_acc: 0.4468\n",
      "Epoch 128/150\n",
      "6279/6279 [==============================] - 2s 392us/sample - loss: 1.3576 - acc: 0.4004 - val_loss: 1.6282 - val_acc: 0.4672\n",
      "Epoch 129/150\n",
      "6279/6279 [==============================] - 2s 392us/sample - loss: 1.3220 - acc: 0.4163 - val_loss: 1.5384 - val_acc: 0.4780\n",
      "Epoch 130/150\n",
      "6279/6279 [==============================] - 2s 386us/sample - loss: 1.3050 - acc: 0.4177 - val_loss: 1.6122 - val_acc: 0.4697\n",
      "Epoch 131/150\n",
      "6279/6279 [==============================] - 2s 384us/sample - loss: 1.3576 - acc: 0.4165 - val_loss: 1.6099 - val_acc: 0.4697\n",
      "Epoch 132/150\n",
      "6279/6279 [==============================] - 2s 388us/sample - loss: 1.3231 - acc: 0.4029 - val_loss: 1.6060 - val_acc: 0.4793\n",
      "Epoch 133/150\n",
      "6279/6279 [==============================] - 2s 387us/sample - loss: 1.3492 - acc: 0.4134 - val_loss: 1.5672 - val_acc: 0.4678\n",
      "Epoch 134/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.3503 - acc: 0.4125 - val_loss: 1.5803 - val_acc: 0.4710\n",
      "Epoch 135/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.3226 - acc: 0.4235 - val_loss: 1.5482 - val_acc: 0.4691\n",
      "Epoch 136/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.3556 - acc: 0.4117 - val_loss: 1.7001 - val_acc: 0.4315\n",
      "Epoch 137/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.3564 - acc: 0.4184 - val_loss: 1.6300 - val_acc: 0.4487\n",
      "Epoch 138/150\n",
      "6279/6279 [==============================] - 3s 413us/sample - loss: 1.3792 - acc: 0.4106 - val_loss: 1.5287 - val_acc: 0.5105\n",
      "Epoch 139/150\n",
      "6279/6279 [==============================] - 2s 385us/sample - loss: 1.3362 - acc: 0.4098 - val_loss: 1.5803 - val_acc: 0.4512\n",
      "Epoch 140/150\n",
      "6279/6279 [==============================] - 2s 398us/sample - loss: 1.3263 - acc: 0.4228 - val_loss: 1.6371 - val_acc: 0.4583\n",
      "Epoch 141/150\n",
      "6279/6279 [==============================] - 2s 389us/sample - loss: 1.3089 - acc: 0.4173 - val_loss: 1.6065 - val_acc: 0.4321\n",
      "Epoch 142/150\n",
      "6279/6279 [==============================] - 3s 400us/sample - loss: 1.3124 - acc: 0.4120 - val_loss: 1.5688 - val_acc: 0.4328\n",
      "Epoch 143/150\n",
      "6279/6279 [==============================] - 2s 392us/sample - loss: 1.3232 - acc: 0.4161 - val_loss: 1.6874 - val_acc: 0.4366\n",
      "Epoch 144/150\n",
      "6279/6279 [==============================] - 2s 397us/sample - loss: 1.3593 - acc: 0.4173 - val_loss: 1.6235 - val_acc: 0.4621\n",
      "Epoch 145/150\n",
      "6279/6279 [==============================] - 2s 393us/sample - loss: 1.3431 - acc: 0.4109 - val_loss: 1.6206 - val_acc: 0.4347\n",
      "Epoch 146/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.3521 - acc: 0.4118 - val_loss: 1.5548 - val_acc: 0.4844\n",
      "Epoch 147/150\n",
      "6279/6279 [==============================] - 2s 394us/sample - loss: 1.3229 - acc: 0.4111 - val_loss: 1.5495 - val_acc: 0.4474\n",
      "Epoch 148/150\n",
      "6279/6279 [==============================] - 2s 391us/sample - loss: 1.3655 - acc: 0.4013 - val_loss: 1.5640 - val_acc: 0.4519\n",
      "Epoch 149/150\n",
      "6279/6279 [==============================] - 2s 396us/sample - loss: 1.3124 - acc: 0.4168 - val_loss: 1.5853 - val_acc: 0.4442\n",
      "Epoch 150/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6279/6279 [==============================] - 2s 395us/sample - loss: 1.3276 - acc: 0.4130 - val_loss: 1.5548 - val_acc: 0.4538\n",
      "6279\n",
      "1569\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = np.asarray(full_train)\n",
    "y = np.asarray(train_labels)\n",
    "\n",
    "\n",
    "models = np.array(())\n",
    "fits = np.array(())\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    \n",
    "    \n",
    "    train_x = X[train_indices]\n",
    "    eval_x = X[test_indices]\n",
    "    \n",
    "    \n",
    "    train_y = y[train_indices]\n",
    "    eval_y = y[test_indices]\n",
    "    \n",
    "    \n",
    "    wtablelabs = np.asarray(all_labs)\n",
    "    wtable = get_wtable(wtablelabs)\n",
    "    \n",
    "    newmodel = new_model(train_x)\n",
    "    \n",
    "    \n",
    "    newmodel.compile(optimizer='adam', loss= mywloss, metrics=['accuracy'])\n",
    "    fit = newmodel.fit(train_x, train_y, validation_data = [eval_x,eval_y], epochs = 150, batch_size = 20, shuffle = True)\n",
    "    \n",
    "    models = np.append(models,newmodel)\n",
    "    fits = np.append(fits,fit)\n",
    "    \n",
    "    \n",
    "    print(len(train_indices))\n",
    "    print(len(test_indices))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = np.array(())\n",
    "\n",
    "for fit in fits:\n",
    "    \n",
    "    length = len(fit.history['val_loss'][1:])\n",
    "  \n",
    "\n",
    "\n",
    "    lastfiveval = fit.history['val_loss'][(length - 4):length + 1]\n",
    "    lastfivetrain = fit.history['loss'][(length - 4):length + 1]\n",
    "\n",
    "    \n",
    "    evaluation = np.mean(lastfiveval) + np.mean(lastfivetrain)\n",
    "    evaluations = np.append(evaluations,evaluation)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select best model\n",
    "\n",
    "min_eval = np.min(evaluations)\n",
    "\n",
    "\n",
    "for i in range(len(evaluations)):\n",
    "    \n",
    "    if(evaluations[i] == min_eval):\n",
    "        \n",
    "        ind = i\n",
    "        break\n",
    "    \n",
    "\n",
    "best_model = models[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchnums = [\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_meta = pd.read_csv(\"./test_set_metadata.csv\")\n",
    "#test_data = pd.read_csv(\"./test_set_batch2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test,test_ids = process_test(test_data,test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting bn =  2\n",
      "starting read\n",
      "starting process\n",
      "doing agg\n",
      "doing first batch of aggregations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\15173\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing second batch of aggregations, passbands\n",
      "done with hard engineering\n",
      "doing merge\n",
      "starting prediction\n",
      "done wiht bn =  2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_predictions(\"test_pred.csv\",batchnums,best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
